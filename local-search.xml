<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于网内聚合的分布式机器学习加速策略研究</title>
    <link href="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/"/>
    <url>/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><ul><li>研究背景<ul><li>大规模神经网络训练需求激增，单机训练效率低，而PS-Worker、AllReduce存在显著缺陷<ul><li>PS-Worker：参数服务器带宽瓶颈限制规模扩展</li><li>AllReduce：通信延迟随节点数线性增长</li></ul></li><li>网内聚合（ In-Network Aggregation）成为了加速分布式机器学习训练的新方向<ul><li>通过可编程交换机将梯度聚合卸载到网络层，减少主机计算压力与网络流量</li><li>受限于交换机内存与计算能力</li></ul></li></ul></li><li>核心问题<ul><li>问题1:<ul><li>多任务争抢可编程交换机有限的内存资源，而导致网内聚合速度减慢和交换机内存利用率降低</li></ul></li><li>问题2<ul><li>网内聚合扩展规模受限于可编程交换机有限的计算能力，大规模分布式训练中部署难度和成本高，以及现有网内聚合扩展策略没有充分利用机内高带宽资源</li></ul></li></ul></li><li>解决方案<ul><li>问题1：RA-INA混合同步算法<ul><li>动态共享交换机内存，梯度分组后优先执行Ring-AllReduce</li><li>数据包到达交换机时批量抢占聚合器，成功则切换网内聚合</li></ul></li><li>问题2：CINA链式扩展策略<ul><li>将 ToR（Top of Rack）交换机替换为可编程交换机，在 ToR层形成一条网内聚合的链式流水线</li><li>利用机内高带宽，在机内进行 Ring-Reduce将梯度聚合到机内主节点上，协同流水线进行机间的网内聚合</li></ul></li></ul></li><li>关键词<ul><li>分布式机器学习</li><li>可编程交换机</li><li>网内聚合</li><li>多任务</li><li>大规模训练 # 第一章 绪论</li></ul></li><li>研究背景及意义<ul><li>核心问题<ul><li>大规模神经网络训练（如ChatGPT）需求激增，单机训练耗时过长（如1024块A100训练34天），分布式训练成关键技术。</li></ul></li><li>通信瓶颈<ul><li>PS-Worker​<ul><li>参数服务器（PS）节点多对一通信导致带宽瓶颈</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/1.png" alt="PS架构瓶颈"><figcaption aria-hidden="true">PS架构瓶颈</figcaption></figure></li></ul></li><li>AllReduce<ul><li>通信延迟随节点数线性增长，长环结构受限于最低带宽链路​</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/2.png" alt="AllReduce瓶颈"><figcaption aria-hidden="true">AllReduce瓶颈</figcaption></figure></li></ul></li></ul></li><li>网内聚合价值<ul><li>通过可编程交换机卸载梯度聚合，减少网络流量与通信跳数，但受限于交换机内存与计算能力</li></ul></li></ul></li><li>研究现状<ul><li>中心化并行训练（PS-Worker）<ul><li>工作流程<ul><li>作为中心化的 PS 将在本地维护一个全局模型， 负责更新 Worker上的本地模型。 在每次迭代训练中，训练数据集会被分割并分配给每个 Worker进行本地训练，完成训练后计算梯度值并将其推送给 PS。 PS 收到来自各个Worker 的梯度后，使用随机梯度下降法或其他优化算法更新全局模型。随后Worker 会从 PS 拉取最新的模型以便进入下一次迭代训练。</li></ul></li><li>演进历程<ul><li>第一代<ul><li>基于Memcached的分布式参数存储</li></ul></li><li>第二代<ul><li>DistBelief</li></ul></li><li>第三代<ul><li>PS-Lite通用架构</li></ul></li></ul></li><li>优化方案<ul><li>数据/模型并行<ul><li>SINGA</li><li>CNTK</li></ul></li><li>重叠计算/通信<ul><li>WFBP</li><li>BytePS</li></ul></li><li>弹性参数服务器<ul><li>EPS</li><li>Pathways</li></ul></li></ul></li></ul></li><li>去中心化并行训练（AllReduce）<ul><li>工作流程<ul><li>不同于参数服务器模式需要 PS 和Worker 两类工作节点，在 AllReduce模式中只需要 Worker 节点， 模型参数或梯度只在 Worker 之间传输，每个Worker 都是平等的</li><li>将所有 Worker连接成一个逻辑环，每个 Worker把本地计算得到的梯度划分成 N份并依次把自己的梯度同步给下一个邻居Worker，总共经过 2*(N-1)轮同步，才能够完成所有 Worker 的梯度更新</li></ul></li><li>优化方案<ul><li>分层同步<ul><li>Hierarchical-AllReduce</li><li>2D-Torus AllReduce</li><li>HiPS</li></ul></li><li>异构问题<ul><li>BlueConnect</li><li>Blink</li><li>FlexReduce</li><li>DS-Sync</li></ul></li></ul></li></ul></li><li>网内聚合技术<ul><li>基于可编程网络设备（如可编程交换机、 FPGA 或智能网卡等）的计算能力加速各种聚合应用</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/3.png" alt="常见网内聚合解决方案对比"><figcaption aria-hidden="true">常见网内聚合解决方案对比</figcaption></figure></li></ul></li></ul></li><li>研究内容<ul><li>基于 Ring-AllReduce 与网内聚合的混合同步算法</li><li>大规模分布式机器学习训练的网内聚合CINA链式扩展策略</li></ul></li></ul><h1 id="第二章-相关技术概述">第二章 相关技术概述</h1><ul><li>分布式并行训练策略<ul><li>数据并行训练<ul><li>核心原理​<ul><li>数据集切分到不同计算节点，每个节点持有完整模型副本进行本地训练</li></ul></li><li>同步机制<ul><li>PS-Worker模式：Worker推送梯度至PS，PS聚合后广播更新（易带宽瓶颈）</li><li>AllReduce模式：节点间直接同步梯度（无中心节点）</li></ul></li><li>挑战<ul><li>通信开销随节点数增长，可能引发掉队者问题</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/4.png" alt="分布式数据并行训练"><figcaption aria-hidden="true">分布式数据并行训练</figcaption></figure></li></ul></li><li>模型并行训练<ul><li>核心原理<ul><li>模型按层/神经元切分到不同节点</li></ul></li><li>划分策略<ul><li>横向按层划分​<ul><li>节点负责特定网络层（层数多时适用）</li></ul></li><li>纵向跨层划分<ul><li>单层参数矩阵分块（神经元多时适用）</li></ul></li><li>混合划分<ul><li>结合横向与纵向策略</li></ul></li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/5.png" alt="分布式模型并行训练"><figcaption aria-hidden="true">分布式模型并行训练</figcaption></figure></li></ul></li></ul></li><li>网内聚合<ul><li>可编程交换机<ul><li>核心架构<ul><li>RMT（可重构匹配表）模型支撑数据包处理</li></ul></li><li>关键部件<ul><li>解析器​：提取/修改数据包头部字段</li><li>匹配部件​：执行流表规则（匹配+动作集）</li><li>编程语言：P4定义数据包处理流程</li></ul></li><li>挑战限制<ul><li>聚合流量可能高达数十个 Gb 甚至 Tb，远超 RMT 交换机的内存容量</li><li>SRAM/TCAM内存仅数十MB，需精细管理</li></ul></li></ul></li><li>网内聚合的应用<ul><li>分布式机器学习<ul><li>减少网络流量与主机计算负担</li></ul></li><li>类MapReduce应用<ul><li>使用网内聚合在交换机侧完成部分数据的规约任务， 在 Reduce阶段减少发送给 Reducer 节点的数据， 降低 Incast 现象出现的概率</li></ul></li><li>分布式存储修复<ul><li>减少数据传输量、提高修复效率、降低存储节点负载</li></ul></li></ul></li></ul></li><li>集合通信及分布式训练框架<ul><li>集合通信<ul><li>Broadcast<ul><li>单节点数据分发至所有节点</li></ul></li><li>Gather/AllGather​<ul><li>收集所有节点数据</li></ul></li><li>Reduce/AllReduce<ul><li>跨节点数据聚合（求和/最大值等）</li></ul></li><li>Scatter/Reduce-Scatter<ul><li>数据分块分发与局部聚合</li></ul></li></ul></li><li>分布式训练框架<ul><li>TensorFlow​<ul><li>数据流图架构，支持gRPC通信</li><li>高级API与低级API灵活适配</li></ul></li><li>PyTorch<ul><li>动态计算图，支持GLOO/MPI/NCCL通信后端</li></ul></li><li>MXNet<ul><li>跨语言支持，轻量级分布式训练 # 第三章 基于 Ring-AllReduce与网内聚合的混合同步算法设计</li></ul></li></ul></li></ul></li><li>问题分析<ul><li>Ring-AllReduce 同步算法<ul><li>算法逻辑<ul><li>所有计算节点在逻辑拓扑上形成一个环，计算节点反向传播结束得到梯度后，会将梯度均匀切分成n份。每个节点错开将其中一份梯度发送给右邻居，并接收左邻居的一份梯度与本地梯度进行聚合。然后每个节点再将上一步聚合好的一份梯度继续发送给右邻居，重复上一步的操作。在进行n-1步后，每个节点都将得到一份聚合了所有节点梯度的结果。之后每个节点继续将聚合结果发送给右邻居，同时接收左邻居的聚合结果并覆盖本地位置的梯度。然后每个节点再将上一步接收的聚合结果继续发送给右邻居，重复上一步的操作。在进行n-1 步后，每个节点都将得到整份聚合了所有节点梯度的结果</li></ul></li><li>优势<ul><li>通信量固定（不超过2倍模型大小）</li></ul></li><li>缺陷<ul><li>通信延迟随节点数线性增长</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/6.png" alt="Ring-AllReduce 同步算法示意图"><figcaption aria-hidden="true">Ring-AllReduce同步算法示意图</figcaption></figure></li></ul></li><li>网内聚合算法<ul><li>算法逻辑<ul><li>计算节点将梯度打包发送给可编程交换机，可编程交换机接收到梯度数据包之后寻找到对应的聚合器进行聚合。当聚合器识别到聚合完所有计算节点的梯度数据包之后，将对聚合结果进行打包并广播给所有计算节点</li></ul></li><li>优势<ul><li>将主机侧的聚合操作卸载到交换机侧，在网络中减少聚合流量，降低通信开销</li></ul></li><li>缺陷<ul><li>多任务争抢交换机内存导致效率下降</li></ul></li><li>内存分配机制对比<ul><li>静态固定内存分配<ul><li>每个任务在可编程交换机中平均分配一块内存，任务之间相互隔离，互不影响</li><li>内存利用率低</li></ul></li><li>动态共享内存分配<ul><li>内存由多个任务共享，每个任务的梯度数据包按照先到先服务的机制占用聚合器。此外ESA 还能通过优先级机制抢占已被占用的聚合器</li><li>需要PS节点容错，有可能将回退成 PS-Worker 的训练模式</li></ul></li><li>在多训练任务的场景下采用动态共享内存分配模式更能充分发挥网内聚合的性能</li></ul></li></ul></li></ul></li><li>混合同步算法设计(RA-INA)<ul><li>整体概述<ul><li>主要流程<ol type="1"><li>初始化工作，在同步开始时，可编程交换机将内存划分成大小相同的聚合器，每个聚合器负责一组梯度的聚合任务。计算节点将梯度切分成大小与聚合器内存相同的梯度块，并将梯度块分组，然后对按组将梯度块打包成梯度数据包发送到可编程交换机中进行网内聚合</li><li>当某个节点的梯度数据包在可编程交换机中寻找到连续的 n个聚合器时，该梯度数据包将批量占领这 n个聚合器，并将梯度缓存在对应的聚合器中</li><li>部分梯度块将转而执行网内聚合算法，并在完成所有计算节点的梯度聚合工作之后，可编程交换机会将聚合结果广播给每个计算节点</li><li>若没有在可编程交换机中寻找到满足要求的聚合器组，则该梯度数据包所在的梯度组将继续执行算法的下一步</li><li>如果之后 算法执行到 n-1 步之后，即AllGather 操作时，该梯度组将不再尝试寻找空闲聚合器， 继续完成 AllGather 操作</li></ol></li><li>核心理念<ul><li>动态混合执行<ul><li>梯度同步过程在 ​Ring-AllReduce​ 与​网内聚合间动态切换，通过交换机内存占用状态决策执行路径</li></ul></li><li>批量抢占机制<ul><li>梯度数据包到达交换机时，尝试批量占领与计算节点数相等的聚合器组（非单个抢占），成功则切换网内聚合</li></ul></li></ul></li><li>过程示例<ul><li>单任务<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/7.png" alt="单任务时 RA-INA 算法的同步过程示例"><figcaption aria-hidden="true">单任务时 RA-INA算法的同步过程示例</figcaption></figure></li></ul></li><li>多任务<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/8.png" alt="多任务时 RA-INA 算法的同步过程示例"><figcaption aria-hidden="true">多任务时 RA-INA算法的同步过程示例</figcaption></figure></li></ul></li></ul></li></ul></li></ul></li><li>主机侧逻辑设计<ul><li>数据预处理<ul><li>梯度切分<ul><li>按 ​64个梯度元素/块切分</li><li>n个梯度块为一组（n=节点数），形成逻辑分组</li></ul></li><li>数据包格式<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/9.png" alt="梯度数据包的包格式"><figcaption aria-hidden="true">梯度数据包的包格式</figcaption></figure></li></ul></li></ul></li><li>通信控制逻辑<ul><li>滑动窗口<ul><li>每组梯度独立维护状态机，完成一组后滑动至下一组</li></ul></li><li>浮点处理<ul><li>梯度缩放为32位整数传输，交换机聚合后主机侧还原为浮点数</li></ul></li><li>ACK响应逻辑<ul><li>接收聚合结果后返回ACK，触发交换机释放聚合器</li><li>超时未收到ACK则重传数据包</li></ul></li></ul></li></ul></li><li>交换机侧逻辑设计<ul><li>聚合器结构设计<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/10.png" alt="聚合器结构"><figcaption aria-hidden="true">聚合器结构</figcaption></figure></li></ul></li><li>处理流程<ul><li>聚合器匹配<ul><li>根据包头<code>job_id</code>和<code>group_index</code>定位聚合器组​</li><li>若未占用，遍历寻找连续n个空闲聚合器​</li></ul></li><li><strong>数据聚合</strong>​<ul><li>校验<code>bitmap</code>防止重复聚合</li><li>整型梯度累加到<code>data</code>区，更新<code>bitmap</code>和<code>counter</code></li></ul></li><li><strong>广播触发</strong><ul><li>当<code>counter == n</code>且<code>bitmap</code>全1时，广播聚合结果</li></ul></li></ul></li></ul></li><li>可靠性设计<ul><li>丢包处理机制<ul><li>主机→交换机丢包<ul><li>2MSL未收到ACK</li><li>重传梯度数据包</li></ul></li><li>交换机→主机丢包<ul><li>2MSL未收到结果ACK</li><li>交换机重发聚合结果</li></ul></li><li>ACK丢包<ul><li>重复数据包触发bitmap校验</li><li>丢弃重复包，补发ACK</li></ul></li></ul></li></ul></li><li>实验分析<ul><li>网内聚合性能影响因素分析<ul><li>内存利用率高</li><li>任务数影响小</li><li>内存不足性能衰减小</li></ul></li><li>单任务时的训练吞吐量对比<ul><li>比Ring-AllReduce ​吞吐量提升57%</li><li>比PS-Worker 加速2.0倍</li></ul></li><li>多任务时的平均任务完成时间对比<ul><li>RA-INA比SwitchML ​JCT降低25.4%</li><li>RA-INA比Ring-AllReduce ​JCT降低18.1%</li></ul></li></ul></li></ul><h1 id="第四章-大规模分布式训练的网内聚合扩展策略设计">第四章大规模分布式训练的网内聚合扩展策略设计</h1><ul><li>问题分析<ul><li>HINA分层网内聚合扩展策略<ul><li>同步流程<ul><li>首先所有服务器中的计算节点将梯度数据包向上发送到各自所在的 ToR交换机进行第一次网内聚合。 之后 ToR交换机继续向上将聚合好的梯度数据包发送给 ToR 交换机所连接的 AGG 交换机，进行第二次网内聚合。 然后 AGG交换机仍然将聚合好的梯度数据包向上发送给所连接的Core交换机进行最后一次网内聚合。 最后作为根节点的 Core交换机将得到所有计算节点梯度数据包的聚合结果， 并原路进行组播。在每一层中，该层的交换机都将进一步组播梯度数据包，最终到达每个计算节点，完成本次梯度同步</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/11.png" alt="HINA 在 Fat-Tree 网络拓扑中构建的聚合树"><figcaption aria-hidden="true">HINA 在 Fat-Tree网络拓扑中构建的聚合树</figcaption></figure></li></ul></li><li>缺陷<ul><li>需替换多层级交换机（ToR+AGG+Core），部署成本高</li></ul></li></ul></li><li>MTINA多树网内聚合扩展策略<ul><li>同步流程<ul><li>MTINA 将根据参与训练任务的计算节点分布情况，以 Core交换机为根节点初始化生成多棵聚合树。 与 HINA 中生成的聚合树相同， MTINA中每棵聚合树的叶子节点均为计算节点，非叶子节点均为具有计算功能的可编程交换机</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/12.png" alt="MTINA 在 Fat-Tree 网络拓扑中构建的聚合树"><figcaption aria-hidden="true">MTINA 在 Fat-Tree网络拓扑中构建的聚合树</figcaption></figure></li></ul></li><li>缺陷<ul><li>构建多聚合树需全网可编程交换机，成本激增</li></ul></li></ul></li></ul></li><li>链式网内聚合扩展策略设计<ul><li>整体结构(以Spine Leaf 网络架构为例)<ul><li>网络拓扑​<ul><li>Spine-Leaf架构，仅替换Leaf层为可编程交换机</li></ul></li><li>四阶段流水线：<ol type="1"><li>机内Ring-Reduce​<ul><li>GPU间高带宽聚合梯度至Master Node</li></ul></li><li>机架内INA<ul><li>ToR交换机聚合本机架Master Node数据</li></ul></li><li>机架间链式聚合​<ul><li>ToR交换机形成流水线，跨机架迭代聚合</li></ul></li><li>结果组播<ul><li>最终聚合结果广播回Master Node</li></ul></li></ol></li></ul></li><li>机内聚合策略<ul><li>主节点选择​<ul><li>规则<ul><li>每台服务器固定选择末位GPU为Master Node</li></ul></li><li>作用<ul><li>作为机内聚合结果缓存点与机间通信代理。</li></ul></li></ul></li><li>Ring-Reduce流水线​<ul><li>执行流程<ul><li>梯度切分<ul><li>每GPU梯度分k块</li></ul></li><li>Reduce-Scatter​<ol type="1"><li><span class="math inline"><em>G</em><sub>(<em>i</em>, 0)</sub> → <em>G</em><sub>(<em>i</em>, 1)</sub></span>发送块0，同时接收<span class="math inline"><em>G</em><sub>(<em>i</em>, 3)</sub> → <em>G</em><sub>(<em>i</em>, 0)</sub></span>块3</li><li><span class="math inline"><em>G</em><sub>(<em>i</em>, 1)</sub> → <em>G</em><sub>(<em>i</em>, 2)</sub></span>发送块1，同时接收<span class="math inline"><em>G</em><sub>(<em>i</em>, 0)</sub> → <em>G</em><sub>(<em>i</em>, 1)</sub></span>块0</li><li>…</li></ol></li><li>聚合终点​<ul><li>经(n+k-2)步，所有梯度块聚合至<span class="math inline"><em>G</em><sub>(<em>i</em>, 3)</sub></span></li></ul></li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/13.png" alt="CINA 算法使用 4 个 GPU 进行 Ring-Reduce 操作的分解过程"><figcaption aria-hidden="true">CINA 算法使用 4 个 GPU 进行 Ring-Reduce操作的分解过程</figcaption></figure></li></ul></li><li>延迟公式​<ul><li><span class="math inline">$T_{机内RR} = \frac{(n+k-2)K}{kB_{dtd}} +(n+k-2)\alpha_{dtd}$</span>（n=GPU数,K=梯度大小n=GPU数,K=梯度大小n=GPU数,K=梯度大小）</li></ul></li></ul></li><li>机架内聚合策略<ul><li>ToR交换机<ul><li>核心功能<ul><li>聚合本机架内所有Master Node梯度</li><li>生成机架级聚合结果</li></ul></li></ul></li><li>通信重叠优化<ul><li>Master Node在接收第一份聚合梯度后立即发送至ToR交换机</li><li>覆盖机内剩余通信时间<ul><li><span class="math inline">$T_{重叠} \leq T_{机内RR} -\left(n-1)\times( \frac{K}{kB_{htl}} + \alpha_{htl} \right)$</span></li></ul></li></ul></li></ul></li><li>机架间聚合策略<ul><li>链式聚合原理<ul><li>Leaf交换机按Pod编号串联</li><li>工作流程<ol type="1"><li><span class="math inline"><em>L</em><em>S</em><sub>0</sub></span>：发送本地聚合结果<span class="math inline"><em>g</em>2<sub><em>t</em></sub><sup>(0)</sup></span>​至<span class="math inline"><em>L</em><em>S</em><sub>1</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>1</sub></span>​：计算<span class="math inline"><em>g</em>3<sub><em>t</em></sub><sup>(1)</sup>​ = <em>g</em>2<sub><em>t</em></sub><sup>(1)</sup>​ + <em>g</em>2<sub><em>t</em></sub><sup>(0)</sup></span>，发送至<span class="math inline"><em>L</em><em>S</em><sub>2</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>2</sub></span>​：计算<span class="math inline"><em>g</em>3<sub><em>t</em></sub><sup>(2)</sup>​ = <em>g</em>2<sub><em>t</em></sub><sup>(2)</sup>​ + <em>g</em>3<sub><em>t</em></sub><sup>(1)</sup></span>，发送至<span class="math inline"><em>L</em><em>S</em><sub>3</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>3</sub></span>​：获得全局聚合结果<span class="math inline"><em>g</em>3<sub><em>t</em></sub>​​</span></li></ol></li></ul></li><li>通信重叠优化<ul><li><span class="math inline">$T_{CINA}=T_{机内RR}-T_{重叠}+\frac{K}{B_{htl}}+\alpha_{htl}+(L-1)\times(\frac{K}{kB_{lts}})+\frac{K}{kB_{min}}+\alpha_{min}$</span></li></ul></li></ul></li></ul></li><li>仿真分析<ul><li>理论通信开销推导与对比<ul><li>Ring-AllReduce<ul><li><span class="math inline">$T_{Ring}=2(N-1)\times(\frac{K}{NB_{min}}+\alpha_{min})$</span></li></ul></li><li>PS-Worker<ul><li><span class="math inline">$T_{PS}=(k+1)\times(\frac{K}{kB_{min}})+2\alpha_{min}$</span></li></ul></li><li>HINA<ul><li><span class="math inline">$T_{HINA}=T_{机内RR}-T_{重叠}+\frac{K}{B_{htl}}+\alpha_{htl}+2\times(\frac{K}{kB_{lts}}+\alpha_{lts})+\frac{K}{kB_{min}}+\alpha_{min}$</span></li></ul></li><li>MTINA<ul><li><span class="math inline">$T_{MTINA}=(k+1)\times(S+1)\times(\frac{K}{kSB_{min}})+(S+1)\times\alpha_{min}$</span></li></ul></li><li>优势对比<ul><li>CINA只在服务器与上层交换机间存在多对一通信的压力，并不会一层一层的积累通信压力</li><li>CINA 不需要像 HINA 和 MTINA 中将 Leaf 层和 Spine层的交换机替换为可编程交换机，本设计只需要更换 Leaf层的交换机，降低了部署难度和成本</li><li>三层网络最大支持节点数：​CINA 14,338 &gt; HINA 3,600</li><li>相同的节点规模下， CINA 的部署成本也比 HINA 和 MTINA 更有优势</li></ul></li></ul></li><li>仿真平台搭建<ul><li>平台架构设计​<ol type="1"><li>真实VGG19/ResNet50训练数据</li><li>NS3构建可扩展Spine-Leaf网络。</li><li>实现五种对比算法</li><li>丢包率与聚合正确性监控</li></ol></li><li>关键参数配置<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/18.png" alt="仿真参数配置"><figcaption aria-hidden="true">仿真参数配置</figcaption></figure></li></ul></li><li>Trace激励源设计​<ul><li>PyTorch Profiler + 自定义Hook</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/19.png" alt="仿真平台搭建流程"><figcaption aria-hidden="true">仿真平台搭建流程</figcaption></figure></li></ul></li><li>仿真结果与分析<ul><li>训练时间对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/14.png" alt="VGG19 和 ResNet50 的训练时间对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练时间对比</figcaption></figure></li></ul></li><li>吞吐量对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/15.png" alt="VGG19 和 ResNet50 的训练吞吐量对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练吞吐量对比</figcaption></figure></li></ul></li><li>拓展效率对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/16.png" alt="VGG19 和 ResNet50 的训练拓展效率对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练拓展效率对比</figcaption></figure></li></ul></li><li>加速比对比<ul><li><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/17.png" alt="VGG19 和 ResNet50 的训练加速比对比"> #第五章 总结与展望</li></ul></li></ul></li></ul></li><li>总结<ul><li>RA-INA混合同步算法<ul><li>解决痛点<ul><li>多任务争抢交换机内存导致的效率下降</li></ul></li><li>工作机制<ul><li>动态共享内存<ul><li>任务按梯度组抢占聚合器</li></ul></li><li>混合执行逻辑​<ul><li>Ring-AllReduce容错 + 网内聚合加速</li></ul></li></ul></li></ul></li><li>CINA链式扩展策略<ul><li>解决痛点<ul><li>大规模分布式训练场景下网内聚合的扩展性问题</li></ul></li><li>工作机制<ul><li>多阶段流水线<ul><li>机内Ring-Reduce → ToR层链式聚合</li></ul></li><li>扁平化的聚合结构</li></ul></li></ul></li></ul></li><li>展望<ul><li>同步算法对丢包容忍性</li><li>跨交换机甚至跨机架的实机实验平台</li><li>网内聚合应用在分布式模型并行训练场景，研究异步训练下的网内聚合算法设计</li><li>对于其他的可编程网络设备研究其网内聚合的分布式机器学习加速策略</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>在网计算</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>在网计算</tag>
      
      <tag>2025</tag>
      
      <tag>网内聚合</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于在网计算的分布式系统加速方法</title>
    <link href="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/"/>
    <url>/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><ul><li>背景<ul><li>在网计算是后摩尔时代提升算力的有效方法</li><li>分布式系统扩展面临功耗墙、存储墙，搬动数据开销达到整体70%</li></ul></li><li>定义<ul><li>在网计算是将标准应用卸载到网络设备上的计算模式</li><li>In-Network Computing is the offloading of standard applications torun within network devices</li></ul></li><li>意义<ul><li>压缩网络流量，低时延通信，高速计算</li><li>适用于分布式应用加速，如：分布式大模型训练、分布式数据分析、分布式存储系统</li></ul></li><li>与传统方案对比<ul><li>传统方案的现状<ul><li>分层结构各层独立，各自优化，形成生态，性能无法达到最优</li><li>应用异构性较大，形成各自的通信模式</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/1.png"></li></ul></li><li>在网计算的发展空间<ul><li>大型集群计算成为新趋势，集群归一方所有，分层设计的前提不再具备</li><li>各层具备可编程性、可定制性</li><li>集群建设成本高昂，提升效率意义重大 <img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/2.png"></li></ul></li></ul></li><li>发展路线<ul><li>打通二至五层，各应用领域独立设计；在各个应用领域验证并取得收益</li><li>领域间逐步整合，重新设计通信库标准；使应用开发者迁移到新的开发模式</li><li>功能下沉，更新设备，进一步提升性能；促进大规模商业化生产</li></ul></li></ul><h2 id="技术方案">技术方案</h2><ol type="1"><li><p>加速分布式机器学习 【前置信息】</p><ul><li>背景<ul><li>机器学习：自然语言处理、计算机视觉、智能运维</li><li>分布式机器学习：增长的数据集、增长的模型、场景要求(联邦学习)</li></ul></li><li>机器学习训练算法<ul><li>重复迭代计算梯度，更新模型直至收敛</li><li>分布式训练：每个worker计算梯度 –&gt; 所有梯度聚合并返回给worker–&gt; 更新模型</li></ul></li><li>分布式训练参数服务器（Parameter Server, PS）<ul><li>假设：N个worker，梯度大小为M</li><li>通信量 worker: M PS: N * M 【问题与目标】</li></ul></li><li>问题<ul><li>PS链路成为瓶颈</li></ul></li><li>目标<ul><li>主要目标：使用在网计算加速分布式训练</li><li>其他目标：兼容性、多任务、跨机柜</li></ul></li></ul><p>【方案1】交换机取代PS (NetReduce)</p><ul><li>目标<ul><li>主要目标： 使用在网计算加速分布式训练</li><li>其他目标：与RDMA兼容</li></ul></li><li>PS的类型及工作方式<ul><li>服务器<ol type="1"><li>每个worker把数据发送到PS</li><li>PS把数据求和</li><li>PS将结果广播给worker</li></ol></li><li>交换机<ol type="1"><li>交换机能流式处理报文，但是不能缓存大块数据</li><li>交换机没有四层协议保证数据块完整</li></ol></li></ul></li><li>体系结构<ul><li>Worker：梯度消息被组织为一个报文序列，维护一个滑动窗口发送报文，假设窗口最大值为W</li><li>交换机：内存组织为一个聚合器数组，大小为N <img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/3.png"></li></ul></li><li>工作流程<ol type="1"><li>Worker发送梯度报文，交换机通过聚合器数组实时聚合（agtr.idx =PSN%N）</li><li>一个聚合器中的聚合完成（通过bitmap判断），将结果组播给workers</li><li>worker的滑动窗口继续滑动，发送新的报文</li></ol></li><li>性能结果<ul><li>NetReduce提升训练效率，获得在网聚合和RDMA双重性能增益</li><li>训练AlexNet比Ring AllReduce快45%</li></ul></li><li>小结<ul><li>可以用交换机替换PS</li><li>可以做到对传输层透明，与RDMA集成</li><li>内存管理：交换机内存为端侧窗口两倍</li></ul></li></ul><p>【方案2】 聚合传输协议设计ATP</p><ul><li>背景<ul><li>多租户：多任务共享基础设施</li><li>多机柜：BERT-Large训练跨多机柜</li></ul></li><li>目标<ul><li>主要目标：使用在网计算加速分布式训练</li><li>次要目标：支持多租户任务、支持跨机柜部署</li></ul></li><li>内存分配类型<ul><li>静态交换机内存分配<ul><li>划分交换机内存为隔离的区域，分配给多任务</li><li>交换机内存低效率、集成复杂</li></ul></li><li>动态交换机内存分配<ul><li>交换机内存为一个聚合器资源池，对于每个任务的报文采用先到先服务</li><li>去中心化聚合器寻址</li><li>Agtr.idx = Hash(JobID, PSN)</li></ul></li></ul></li><li>体系结构<ul><li>保留PS，寻址可能失败，需要回退机制（Fallback），交换机难以正确处理重传报文</li><li>聚合器，记录JobID、Seq，检测冲突，释放需要由ACK完成</li></ul></li><li>工作流程<ol type="1"><li>worker端维护滑动窗口，发送梯度报文</li><li>梯度报文抵达交换机，进行寻址</li><li>PS接到聚合结果或者透传报文，完成聚合，返回ACK</li><li>ACK抵达交换机，释放聚合器,推动滑动窗口</li></ol></li><li>性能结果<ul><li>通信瓶颈任务吞吐量提升显著，多任务竞争下资源利用率更高</li></ul></li><li>小结<ul><li>统计复用交换机，利用率更高</li><li>正确性机制更复杂</li></ul></li></ul><p>【方案1与方案2对比】</p><ul><li>组成<ul><li>方案1：交换机</li><li>方案2：交换机+PS</li></ul></li><li>寻址机制<ul><li>方案1：冲突避免(模运算)</li><li>方案2：冲突探测(Hash)</li></ul></li><li>内存使用<ul><li>方案1：2倍窗口</li><li>方案2：1倍窗口</li></ul></li><li>内存分配<ul><li>方案1：隔离</li><li>方案2：共享</li></ul></li><li>适用范围<ul><li>方案1：单任务隔离</li><li>方案2：多任务共享</li></ul></li><li>优势<ul><li>方案1：可预测性能，不需PS</li><li>方案2：高资源利用率</li></ul></li></ul></li><li><p>分布式机器学习任务管理 【资源管理】</p><ul><li>应用和系统目标<ul><li>高性能，资源利用率，公平性</li></ul></li><li>策略<ul><li>循环（round robin），多级反馈队列（multi-level feedback queue）</li></ul></li></ul><p>【调度策略】</p><ul><li>最早截止时间优先（Earliest Deadline First）</li><li>优先将交换机内存分配给截止时间早的任务</li><li>剩余内存共享给BE任务和暂时无法满足的MD任务</li><li><span class="math display">$$\begin{array}{c}\text{Time}_{\text{job}} = \text{Epochs} \times \left(\text{Time}_{\text{comp}} +\dfrac{\text{Size}_{\text{model}}}{\text{Throughput}} \right)\\\\\text{Memory}_{\text{switch}} = \text{Throughput} \times\text{RTT}\end{array}$$</span></li></ul><p>【内存管理】</p><ul><li>交换机内存管理器<ul><li>北向接口：内存分配</li><li>南向接口：向终端发送内存区域(offset, size)，向交换机发送规则</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/4.png"></li></ul></li><li>寻址模式<ul><li>agtr.idx ← Hash(PSN, JobID)%Size + Offset</li><li>端侧计算地址，并封装在报文头部</li></ul></li></ul><p>【性能结果】</p><ul><li>交换机内存管理可以有效降低平均任务完成时间，提高服务满足率</li></ul></li><li><p>加速大数据分析系统</p><p>【挑战】</p><ul><li>大数据存在数据流聚合<ul><li>大数据系统中的 ReduceByKey()</li><li>数据库系统中的 sum 、 count 等</li></ul></li><li>大数据系统中的聚合为异步聚合<ul><li>同步聚合(机器学习)值流（value stream）<ul><li>键相同</li><li>键线性排列，可预知</li><li>键次数有界</li></ul></li><li>异步聚合（大数据）键值流（key-value stream)<ul><li>不同流中键不同</li><li>键无序，不可预知</li><li>键次数没有界</li></ul></li></ul></li></ul><p>【目标】</p><ul><li>目标<ul><li>适配不同的应用</li><li>提升系统效率</li><li>有正确性保证</li></ul></li></ul><p>【创新设计】</p><ul><li>提升系统有效吞吐率<ul><li>多元组报文</li><li>二维聚合器数组（Aggregator Array，AA）</li><li>端侧元组顺序重构<ul><li>将键的空间划分为互不重叠的子空间</li><li>Hash(Flow ID) –&gt;子空间</li><li>报文有多个槽位（slot）</li></ul></li><li>提升155倍处理速度</li></ul></li><li>可靠性和正确性<ul><li>基于seen状态位避免重复计算</li></ul></li><li>提升内存使用效率<ul><li>双副本AA动态切换，优先处理高频键</li></ul></li></ul><p>【性能结果】</p><ul><li>词频统计任务耗时比Spark（56核）减少50%以上，CPU利用率显著降低</li></ul></li><li><p>通信库设计 【背景】</p><ul><li>在网计算对于应用（框架）开发者并不友好，学习曲线陡峭，难以集成到分布式系统中</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/5.png"></li></ul><p>【目标】</p><ul><li>实现对开发者友好的通信计算库</li></ul><p>【实现方法】</p><ul><li>INC应用分类：同步聚合、异步聚合、键值读写、共识协议</li><li>统一各分类中的数据格式</li><li>统一交换机中的原语</li><li>统一应用编程接口</li><li>系统集成</li></ul><p>【性能结果】</p><ul><li>节约代码行数</li><li>对比其他INC方案性能接近，对比端侧系统吞吐量更高、时延更低</li></ul></li></ol><h2 id="总结">总结</h2><ul><li>在网计算是有效提升分布式应用效率的一种手段</li><li>在网计算应用场景广泛，也是一个很大的研究空间</li></ul>]]></content>
    
    
    <categories>
      
      <category>在网计算</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>在网计算</tag>
      
      <tag>2025</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p>今天心血来潮，搭了自己的博客，希望长更新，加油。</p><p>在这之前的文章都是我后面搬上去的，为了表示对于时间的尊重，我还是按照原时间排列。</p><p>此文章可以作为姓名墙，要是有幸被您访问，可以在评论区留下你的姓名呀~</p><p>行秋镇文底</p><figure><img src="/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" alt="test"><figcaption aria-hidden="true">test</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>测试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高考一周年的回忆</title>
    <link href="/2025/06/07/%E9%AB%98%E8%80%83%E4%B8%80%E5%91%A8%E5%B9%B4%E7%9A%84%E5%9B%9E%E5%BF%86/"/>
    <url>/2025/06/07/%E9%AB%98%E8%80%83%E4%B8%80%E5%91%A8%E5%B9%B4%E7%9A%84%E5%9B%9E%E5%BF%86/</url>
    
    <content type="html"><![CDATA[<p>2024年6月7日，一切都如往常一般，2024年6月9日，一切画上了最后的句号。</p><p>不知不觉，已经过去1年了，那个战场与我而言就是熟悉又是陌生。我不知道现在的自己是否还能和1年前的自己一样充满自信。这一年里，晚上睡觉的时候，我有时还能在梦中回到那个高三4班，回到进才楼，回到那个第一排讲台旁边的位置，回到与玉芬、喜哼、王博欢笑的场景。但这终究是梦，梦是回不去的。更让人害怕的是，随着时间的推移，能梦到的过去会越来越稀薄，直到有一天，过去在梦中也成为了过去，高考就真的只是口中的一个被赋予了各种回忆的抽象集合的代名词了。很害怕，那个时候，当我谈起自己的高考的时候，是否也会像自己的师长那般，想说却说不出任何自己的细节了。</p><p>高考，青春这场大戏的最高潮，再也回不去了。浪潮之后的我们，不再是当年那个只会做题的你我了——再见了，高考。</p><p>怀念是个美丽的骗子，它只给你看过去的种种高光，却隐藏了那些同样重要却不被人注意的细节。当你翻看高考前的旧照片，回放曾经的旧场景，然后开始说服自己——那些在高考前的种种比眼下更好。但是，高考他已经过去了，彻底地过去了，一去不复返了。你的肉体在当下，心仍然活在那个早已不存在的时光，把当下交给那些早已离去的“你我他”。而你回忆中的“你我他”早已继续了自己的生活。他们在已经没有你的故事里找到了新的平衡，而你却依旧紧握着，死死地把时光攥在手里不想让它离去。</p><p>很遗憾，上面所述的就是我。这一年以来，我一次次回忆起过去的时光，就愈发的感到后悔。如果当时这样这样，我岂不是就可以那样那样了？而这一切都是假设罢了，假设是没有结果的，他让我在一个接着一个的假设中迷失自我。</p><p>寒假与玉芬、喜哼、洛熙一起回了学紫看望老师。又走过了那个鱼龙混杂的进才楼一楼，又走过了共同偷印资料的图书馆，又走过了那个逗仓鼠、兔子的生竞小教室，又走过了偷玩电脑的化竞阶梯教室。那些都是充满了回忆的地方，可是现在呢？当年自我自习狂潮的进才一楼如今再没见到一个新人、当年偷印资料的图书馆一楼被锁上大门、曾经动物多多的场所如今只留下几个空空的笼子、而阶梯教室的黑板上也不再是当年毕韧每周带来的新奇公式。一切都已经逝去了，在那时，我的心中充满了一股空虚感。是啊，斯人斯物皆已逝，我们何时来过、有何时离去呢？</p><p>一年以后的现在，我想对一年前的自己说：“不必这么紧张，放轻松，一场小小的考试而已，决定不了什么”。那个时候在高考前一晚还在疯狂背《种树郭橐驼传》的我肯定无法理解曾经朱振东在课堂上所说的“选择大于努力”一言，一切的一切都需要自己去感悟，去体会。</p><p>让过去留在它该留的地方吧。今天，比过去更需要你！</p><p>这次仅仅只是一个高考一周年小随笔，写的很乱，也是想到那写到哪，还望见谅。</p><div style="text-align: right;">——2025年6月7日星期六1:33于西电宿舍</div>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
      <tag>高考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>首考日的回忆</title>
    <link href="/2025/01/06/%E9%A6%96%E8%80%83%E6%97%A5%E7%9A%84%E5%9B%9E%E5%BF%86/"/>
    <url>/2025/01/06/%E9%A6%96%E8%80%83%E6%97%A5%E7%9A%84%E5%9B%9E%E5%BF%86/</url>
    
    <content type="html"><![CDATA[<p>正如同金工划线，一次划成固然是好的，但当我们第一次划错了，并不代表这个工件已经无用了，相反我们可以细心地为它第二次划线。</p><div style="text-align: right;">——郑盛的最后一课</div><p>早上6点半，我一早来到了2楼实验室，一边啃着面包，一边再过最后一遍物理学史和实验题。在这之前，我虽然已经经历了大大小小好多次模考，但终究没有真正来过一次首考。现在，不到1个半小时，首考它真的来了。</p><p>早上7点出头，该进场了。我和同学们商业互吹着跨过“状元门”，与荣考最后击一下掌，哼着小曲进入了博识楼。因为学选考同时进行的关系，我还能看到好多高二的的xdx，不禁想起一年前的自己也是这样注视着前辈们走进物理考场。</p><p>7点半，我来到了自己的考场，坐到了自己的位置。一切都是如此平静，我的斜前方坐的是“丁神”。默念着“丁神祝我”，我习惯性地准备把笔袋放到抽屉了……然鹅，逆天考场检查，在我座位抽屉里赫然摆着物理教材全解和教科书！心里骂了好几遍清理考场的xdx，我举手示意了一下监考员。监考员带着笑容走了过来，也许以为是一些很平常的事情，然后我就看着她的笑容是如何快速凝固的（嘿嘿）。她也骂了一遍清考场的xdx，然后就把书全掏了出来，放到了讲台上。</p><p>7点50，发卷了，我大致翻了翻，太好了是电磁感应新情境，我们没救了！随着8点铃声一响，开始答题了，我也是拼尽全力开干选择题。嗯，这道牛二，这道平抛，这道宇宙航行，还是很轻松的，太好了是未知量估计压轴，可以看实验了。随着丁神第一个在我们考场翻卷子面了，我也开始被动能定理左一拳右，电磁感应右一脚。</p><p>9点了，高二的xdx放了。听着外面嘈杂的声音，我知道留给自己的只有半个小时了，我该如何收拾这残局呢……</p><p>中午12点，我放弃了午睡，又是啃着面包，还想做一遍镇海的压轴选择。不知怎的，做着做着就进入了经典的“一个人问你问题，然后一群人开始互相问来问去”的故事情节了。我也没有做完自己本来想要完成的静心的计划。心里想着互相提问也算背书，我也加入了聊天大群。</p><p>十二点四五十，又要进考场了，和卫捐又一次的击掌。当时我在心中还想着，这是和您最后一次见面了，拜拜了您嘞。</p><p>下午1点，怀着平静的心情再次进入考场。</p><p>1点20，看到了卷子，我大致扫了一眼，嗯，与考前想的没啥区别。</p><p>1点半，随着铃声我又开始了自己想象中的“冲锋”。很好，经典选择空了好多道。没事，不慌，大题只要不要太差还有救。就决定是你了，无机题，交出你的全部分数！然后……然后，我就被突如其来的计算题单杀了。为什么？！无机化学的部分怎么会出现计算题！可恶的出卷老登！问题不大，继续做反应原理，稳住阵脚！不是，诶呦，你干嘛……怎么要计算酸碱度了……</p><p>2点55，我绞尽脑汁，想出来2个同分异构体。</p><p>化学，不可战胜的。</p><p>刚考完化学，就马上要去考技术了。化学的失利，让我决定要好好再看看技术书。于是乎，看到了3点半。于是乎匆匆忙忙过安检。于是乎顺便和军辉和“昨晚答疑未出现”的牢郑击了下掌。</p><p>16点25，来到了信息部分的最后一个大题，我也相信自己可以摘下这颗明珠。心里想着如此，我便开始了自己的冲锋。</p><p>16点45，面对着还没有头绪的三个空，我选择了先去做通用。但正如军辉所说的那样，在做通用的时候，我脑子总是和我说“你信息最后一道大题没写出来”。但我没有办法，只能硬着头皮做。通用也是出现了新题，祖传的天人合一题目。</p><p>17点10分，杀到了通用最后一题，太好了是用非门做振荡器。想起来了，我一切都想起来了，这个结构我以前做过！等一下，丸辣，忘记咋搞了。我凭着自己的记忆，照猫画虎地画了一个。我知道，这道题，应该没什么人能做出来，毕竟这张卷子好像没多少人写过。但是信息那三个空却能决定生死。</p><p>17点28分，盯出来了，是桶，我们有救了。但是，脑子此时却突然掉线了。</p><p>17点半，技术考试结束。</p><p>2024年的1月6日便是如此结束了。我只记得自己像个孤魂一般，飘出了博识楼，来到了三楼餐厅点了一碗牛肉面。那是我记忆深刻的一天，也许是因为这碗面里面掺杂了我的眼泪吧。或许有不甘，有无奈，有遗憾，有懊悔，原来这才是首考。</p><p>2025年1月6日，恍惚间，距离我首考结束居然已经整整一年了。</p><p>如此巧合，今晚实在想不出来吃啥的我又在西北的这座大城市里给自己点了一碗牛肉面。又是18点多，又是一个人，又是这碗面，还是这团能让我眼睛蒙上白色的雾。今天的我又刚好考完了逆天的大学英语考试。</p><p>此时此刻竟正如彼时彼刻</p><div style="text-align: right;">敬我的2024年浙江首考一周年。</div><div style="text-align: right;">——晚上吃面忽然想起往事的小鲁</div><div style="text-align: right;">2025.1.6</div><div style="text-align: right;">于西电宿舍</div>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
      <tag>高考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的2024</title>
    <link href="/2024/12/31/%E6%88%91%E7%9A%842024/"/>
    <url>/2024/12/31/%E6%88%91%E7%9A%842024/</url>
    
    <content type="html"><![CDATA[<p>很快啊，这样就已经来到了2024年的最后一年（牢大年的最后一天了……）。其实感觉2024年的第一天还在眼前，那天我还特意去径山寺给自己首考祈福，结果偶遇马克西姆，debuff加满，拼尽全力无法战胜（）。</p><p>但这一年也是确确实实过去了，在这个2024年我又经历了无数大大小小的事情。</p><p>一月是浙江特有的首考“快乐”时间。首考也是击碎了我自己对于赋分制的美好幻想，当我还在璃月港乱逛的时候，那份1年前的成绩短信也随之而来，物化的失利的确让人破防，从此同学之间都带上了首考分数的面具。当然如此沉重的一月也是有乐子的，我们也送走了牢郑（whatcan Isay）。就这样以一个传奇人物的陨落为开头，我为期半年的六边形生涯就此开始了。</p><p>二月开始备战三一了。三位一体确实是一个很有意思的东西，我也蛮喜欢的（也许是因为只需要动嘴皮子的原因吧）。但很遗憾，尽管备战了好久三一，最终省内却只报了一个杭电，而且还因为没交报名费的原因幽默出局，现在想想确实很幽默。</p><p>三月到四月就是回归刷刷题的时间了。不同于首考备战，物化技很难再回到过去大家一起竞速的时代，转而代之的便是大数学时代。逆天数学改革，19题确实与过去练习一坤年的21题的卷子不一样，而如今在25年到来之际，我也知道了，数学卷子又双叒叕改了，不过这就是后话了。这段时间，我感觉自己又回到了高三上的时间，和大家玩的玩、笑的笑、乐的乐，玩玩原神，打打万智牌，亦或者与大家讨论讨论人win与joker的逸闻。</p><p>五月拍毕业照了。那段时间，我经常会想到，居然还有不到一个月就要与大家告别了，时间可过的真快啊。这段时间强度拉满，也算是黎明前的黑暗吧。</p><p>六月，真正的高考来了。听了快18年了，从小到大我遇到的师长同龄们都跟我说，这是一场决定人生的考试，也是你一生最公平的考试。在以前我总是会想象当自己真的坐到高考考场上时，那个时候我会怎样？而事实告诉我，我依旧是平静地坐在考场中完成了高中生涯最后六份普普通通的卷子。当我交完化学卷子后，我是一片空白的。因为它的终于结束了，那个预告了18年的大boss终于结束了。我不知道自己当时是怀揣着怎样的心情回到2楼的那个实验室，尽管当时我已经知道化学的同分异构体没有写完。结束了，一切都结束了，我终于不用为解析几何费劲心思了、终于不用每天背作文模板了、终于不用每天做化学选择了。当然之后我也简单了参加了港中深和浙大海宁的三一，不过很显然，身处西安的我已经给出了答案——重在参与。最后在我生日正正好好一个月前，浙江省教育局给我发来了2024年以来第二封短信，出成绩了。</p><p>暑假的开头被志愿填充得焦头烂额，当然也是成功得知了自己的下家——泥电。一切都结束了，至此高中的最后一页也翻过去了，别了我的紫金港。接着就是准备电专的生活了，也是采购了一大堆电子产品，玩了好多游戏（当然也稍微学了一点）。这就是传说中最幸福的暑假！而我的IP也从跟着自己18年的杭州切换至西安服务器了。</p><p>九月又是一个金秋飘香的开学季，而我的朋友们，他们暂时是见不到了，不过我也迎来了新的朋友们。在这个九月我也做了好多18年以前从来没干过的事情，也真正体验到了传说已久的大学生活。</p><p>十月是一个悲伤的月份，就不必回忆了。</p><p>十一，十二月便是在适应大学生活后的时光了。在初来到西电这片土地的兴奋劲过去后，便是我平平淡淡的生活了。乐跑40次，学生组织开会，考试学习以及积极参加各类活动，的的确确时光就是这么过去了，我竟想不出一些刻苦铭心的事情。</p><p>行文至此，我已经简单地写完了自己这一年所经历的事情了。很遗憾，正如同《三体》中三维的物体跌落到二维后便会永远地失去一些细节一般，这一年的经历远远不会像我前面所描述的那样稀稀拉拉。这一年有你我的酸甜苦辣，有你我的点点滴滴。感谢各位在我的2024年剧本中的出现，有的也许是常驻嘉宾，有的也许只相遇在转瞬之间，但正是有了各位的出现，我的2024年才会如此具体丰富。</p><p>愿星河徜徉，未来一路有光。</p><p>各位，2025见！</p><div style="text-align: right;">2024年12月31日凌晨1点50分</div><div style="text-align: right;">鲁铭康</div><div style="text-align: right;">于西安电子科技大学</div><p><img src="/2024/12/31/%E6%88%91%E7%9A%842024/1.png"></p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>年度回忆</tag>
      
      <tag>2024</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
