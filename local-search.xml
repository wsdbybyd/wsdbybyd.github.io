<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ATP：多租户学习中的网络内聚合</title>
    <link href="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/"/>
    <url>/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/</url>
    
    <content type="html"><![CDATA[<h1 id="abstract-摘要">Abstract 摘要</h1><ul><li>核心创新<ul><li>提出ATP（Aggregation Transmission Protocol）服务</li></ul></li><li>目标场景​<ul><li>多租户多机架集群中的分布式深度学习训练</li></ul></li><li>关键技术​<ul><li>可编程交换机实现网络内梯度聚合</li><li>decentralized, dynamic, best-effortaggregation中心化的、动态的、尽力而为的聚合资源分配</li></ul></li><li>性能收益​：<ul><li>训练吞吐提升38%-66%</li></ul></li></ul><h1 id="introduction-引言">1. Introduction 引言</h1><ul><li>问题<ul><li>通信瓶颈转移<ul><li>专用硬件的最新进展已将分布式训练的性能瓶颈从计算转移到通信</li><li>在没有网络通信的情况下，VGG16训练速度可以提高4倍</li></ul></li><li>多租户场景痛点<ul><li>供多机架/多交换机集群中的多个 DT租户利用这一普遍问题，尚未受到系统的关注。</li><li>实现这种服务需要一些机制，以便在多个租户之间共享有限的多交换机聚合资源</li></ul></li></ul></li><li>ATP<ul><li>梯度片段包<ul><li>ATP将每个DT作业的梯度分块成固定大小的片段</li></ul></li><li>聚合器<ul><li>将可编程交换机资源划分为相同大小的片段</li><li>去中心化的聚合器分配机制<ul><li>通过在梯度片段数据包到达交换机时动态分配空闲聚合器，来支持多个作业在线速下进行聚合</li></ul></li><li>专门对交换机逻辑和终端主机网络堆栈进行协同设计，以支持可靠性和有效的拥塞控制</li></ul></li><li>浮点值量化机制</li><li>终端主机的内核旁路设计<ul><li>有的协议栈不会被ATP的网络栈替换，非ATP应用程序可以继续使用现有的协议栈</li></ul></li></ul></li><li>性能<ul><li>多个作业的单机架DNN模型测试平台<ul><li>当只有一半的所需聚合器可用时，性能仅下降5-10％</li><li>当交换机资源竞争激烈时，性能优于当前最先进的技术38％</li></ul></li><li>典型的拓扑结构模拟<ul><li>ATP可减少66％的网络流量</li><li>ATP的丢包恢复机制优于最先进的技术（SwitchML）34％</li><li>具有拥塞控制的ATP作业比没有拥塞控制的作业加速3倍</li></ul></li></ul></li></ul><h1 id="background-and-motivation-背景和动机">2. Background andMotivation 背景和动机</h1><ul><li><h3 id="preliminaries预备知识">Preliminaries（预备知识）</h3><ul><li>PS Architecture(PS架构)<ul><li>Worker本地计算梯度 <span class="math inline">→</span>跨网络传输至PS聚合 <span class="math inline">→</span> 返回更新参数</li><li><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/1.png"></li></ul></li><li>Programmable Switch(可编程交换机)<ul><li>无状态对象<ul><li>元数据，保存每个数据包的瞬态状态，并且当该数据包被丢弃或转发时，交换机释放此对象</li></ul></li><li>有状态对象<ul><li>寄存器，只要交换机程序正在运行，就保持状态。可以在数据平面中读取和写入寄存器值，但对于每个数据包，只能访问一次，无论是读取、写入还是两者都进行</li><li>寄存器是一个值数组。在网络内聚合的上下文中，每个数据包都有一组梯度值，并且需要一组寄存器来聚合它们。我们将这组寄存器称为聚合器</li></ul></li><li>限制<ul><li>寄存器内存只能在交换机程序启动时分配。要更改内存分配，用户必须停止交换机，修改交换机程序并重新启动交换机程序</li><li>计算灵活性受到阶段数量、有效载荷解析能力以及每个阶段的时间预算的限制：只有独立的计算原语可以放置在同一阶段，并且在同一阶段访问的寄存器数量也受到限制</li><li>网络内计算和存储应用的数据包大小较小：SwitchML和NetCache的有效载荷大小为128B</li></ul></li></ul></li><li>In-Network Aggregation(网络内聚合)<ul><li>梯度可以被看作是一系列片段（每个片段都有一部分梯度值），所有梯度的聚合（梯度相加）就是每个片段的聚合。每个片段的网络内聚合都在特定的聚合器中完成</li><li><figure><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/2.png" alt="In-network aggregation example"><figcaption aria-hidden="true">In-network aggregationexample</figcaption></figure></li></ul></li></ul></li><li><h3 id="in-network-aggregation-as-a-service网络内聚合即服务">In-NetworkAggregation as a Service（网络内聚合即服务）</h3><ul><li>SwitchML的静态分配缺陷<ul><li>资源低效<ul><li>应用于多个DT作业时，SwitchML需要对交换机资源进行静态分区，其中每个作业被静态地分配到一个分区</li><li>固定分配聚合器导致任务间歇期（Off-Phase）资源闲置</li></ul></li><li>可拓展性差<ul><li>SwitchML将每个DT作业的梯度聚合完全卸载到机架交换机。在交换机资源严重争用的情况下，DT作业必须等待交换机资源，从而导致从worker到PS的网络链路带宽利用不足</li><li>在网络拓扑的每一层启用聚合服务会使服务设计和网络操作复杂化</li></ul></li></ul></li><li>协议层的挑战<ul><li>端到端语义失效（Rethinking Reliability）<ul><li>在聚合期间，一些数据包在网络内部被消耗。传统的基于终端主机的可靠性机制可能会将网络内数据包消耗误解为数据包丢失，从而导致不必要的重传，并且由于现有可靠性机制无法处理这些新型数据包事件，因此会导致不正确的梯度聚合</li></ul></li><li>拥塞控制失真（Rethinking Congestion-Control）<ul><li>多租户情况下，DT作业可用的网络资源（交换机聚合器和网络带宽）会发生波动</li><li>由于端到端语义被打破，无法使用依赖于RTT或丢包作为拥塞信号的传统拥塞控制算法</li></ul></li></ul></li></ul></li></ul><h1 id="design-设计">3. Design 设计</h1><ul><li><h3 id="atp-overviewatp概述">ATP Overview(ATP概述)</h3><ul><li>ATP VS TCP<ul><li>ATP针对其目标上下文重新设计了特定的传输特性，例如可靠性、拥塞控制和流量控制</li><li>ATP不实现TCP的按序字节流和多路复用抽象，因为它们不适用于目标上下文</li></ul></li><li>梯度分片处理<ul><li>将梯度拆分为固定大小分片，通过<code>JobID</code>和<code>Sequence Number</code>标识</li></ul></li><li>动态聚合决策<ul><li>分片通过哈希映射至交换机聚合器</li><li>资源不可用时直通PS端聚合</li></ul></li><li>两级聚合层级<ul><li>仅在工作节点机架（第一级）和PS机架（第二级）部署聚合，确保每个梯度分片数据包仅被聚合一次</li></ul></li><li>检测处理丢包<ul><li>使用基于超时的重传或来自PS的乱序参数ACK包。当一个包被重传时，它会设置重发标志</li><li>设置ECN标志，ECN拥塞信号经聚合多播至所有Worker</li></ul></li><li><figure><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/3.png" alt="ATP dynamic, best-effort aggregation example"><figcaption aria-hidden="true">ATP dynamic, best-effort aggregationexample</figcaption></figure></li></ul></li><li><h3 id="atp-infrastructure-setupatp基础设施设置">ATP InfrastructureSetup(ATP基础设施设置)</h3><ul><li>静态设置<ul><li>交换机<ul><li>每个可编程交换机安装一个分类器来识别ATP流量</li><li>分配一部分聚合器来聚合ATP流量</li></ul></li><li>终端主机<ul><li>安装一个ATP网络堆栈，该堆栈拦截来自DT作业的所有推送或拉取梯度调用</li><li>了解网络拓扑以及交换机上的聚合器总数，可以协调跨多个交换机的聚合</li></ul></li></ul></li><li>动态作业分配<ul><li>标识分配<ul><li>唯一<code>JobID</code> + <code>WorkerID</code></li></ul></li><li>多播支持​<ul><li>使用IGMP来构建一个多播分发树，供PS将参数返回给worker</li></ul></li></ul></li></ul></li><li><h3 id="data-structures数据结构">Data Structures(数据结构)</h3><ul><li>数据包格式<ul><li>bitmap0 / bitmap1节点聚合位图<ul><li>bitmap0：第一级交换机节点位图<br></li><li>bitmap1：第二级交换机节点位图</li></ul></li><li>fanInDegree0 / fanInDegree聚合完成阈值<ul><li>fanInDegree0：第一级交换机需聚合的分片数</li><li>fanInDegree1：第二级交换机需聚合的分片数</li></ul></li><li>resend重传标记<ul><li>若值为1，此包为重传包，触发聚合器提前释放</li></ul></li><li>collision哈希冲突标记<ul><li>若值为1，发生聚合器索引冲突，PS需触发动态重哈希</li></ul></li><li>ECN显式拥塞通知</li><li>edgeSwitchIdentifier交换机层级标识<ul><li>0：当前包由第一级交换机处理<br></li><li>1：当前包由第二级交换机处理</li></ul></li><li>isAck包类型标识<ul><li>0：梯度包（Worker → PS）<br></li><li>1：参数包（PS → Worker）</li></ul></li><li>JobIDAndSequence分片唯一标识</li></ul></li><li>交换机寄存器<ul><li>bitmap位图记录哪些worker已被聚合</li><li>counter记录已被聚合的workers数量</li><li>ECN指示拥塞情况（只要存在ATP数据包的ECN字段为1，则置1）</li><li>identify由job ID作业ID与sequencenumber序列号组成，唯一标识作业及片段</li><li>timestamp时间戳的更新，发生在执行一次聚合操作时</li><li>aggregator value存储聚合数据</li></ul></li></ul></li><li><h3 id="inter-rack-aggregation机架间聚合">Inter-rackAggregation(机架间聚合)</h3><ul><li>原因<ul><li>仅在worker的本地ToR交换机上进行聚合很简单，但是当worker位于不同的机架中时，会导致到PS的不必要的网络流量</li><li>可以在网络拓扑的更高层级进行聚合。然而，这种方法会大大增加协议的复杂性，因为系统必须处理网络内部的路由更改</li></ul></li><li>ATP部署位置<ul><li>ATP仅在ToR交换机中部署网络内聚合，要么在worker的机架（第一层），要么在PS的机架（第二层）</li></ul></li><li>梯度数据包<ul><li>见数据结构章节</li></ul></li><li><figure><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/4.png" alt="Pseudocode of the switch logic in the ideal case"><figcaption aria-hidden="true">Pseudocode of the switch logic in theideal case</figcaption></figure></li></ul></li><li><h3 id="switch-logic交换机逻辑">Switch Logic(交换机逻辑)</h3><ul><li>Aggregator Allocation 聚合器分配<ul><li>当一个梯度片段数据包到达时，交换机检查数据包的<code>aggregatorIndex</code>字段中聚合器的可用性。终端主机计算 <code>aggregatorIndex</code>为<span class="math inline"><em>H</em><em>A</em><em>S</em><em>H</em>( &lt; <em>J</em><em>o</em><em>b</em><em>I</em><em>D</em>, <em>S</em><em>e</em><em>q</em><em>u</em><em>e</em><em>n</em><em>c</em><em>e</em><em>N</em><em>u</em><em>m</em><em>b</em><em>e</em><em>r</em> &gt; )%<em>n</em><em>u</em><em>m</em><em>A</em><em>g</em><em>g</em><em>r</em><em>e</em><em>g</em><em>a</em><em>t</em><em>o</em><em>r</em><em>s</em></span></li><li>判断<code>&lt;Job ID, Sequence Number&gt;</code>是非为空<ul><li>若为空，将数据包的标识符存储在聚合器中，并将梯度数据包的数据字段复制到聚合器的值字段中。交换机将聚合器中的位图字段从数据包中相应的位图字段复制</li><li>如果聚合器的标识符字段非空，交换机将其与数据包的标识符进行比较<ul><li>如果它们不同，则存在哈希冲突，ATP 将梯度片段数据包向下游推送，以便在PS 处进行处理</li><li>如果聚合器和数据包标识符相等，则发生梯度聚合</li></ul></li></ul></li></ul></li><li>Gradient Aggregation 梯度聚合<ul><li>如果梯度片段数据包有可用的聚合器，ATP 使用<code>edgeSwitchIdentifier</code>从数据包中获取该交换机的扇入度和位图</li><li>ATP通过将数据包的位图与聚合器的位图进行比较来检查该数据包是否已被聚合<ul><li>如果未被聚合，ATP将数据包的梯度数据聚合（相加）到聚合器的值字段中，并将梯度数据包中的位图字段与聚合器中的位图字段进行或运算</li><li>如果数据包已经被聚合，ATP 将数据包中的 ECN 字段与聚合器的 ECN字段进行或运算，并丢弃该数据包</li></ul></li><li>比较计数器与扇入度<ul><li>如果聚合器中的计数器小于相应的扇入度，则 ATP会丢弃梯度片段数据包并对 ECN 进行 OR 操作</li><li>如果它们相等，则此交换机的聚合已完成，可以向下游推送。交换机将数据包的数据字段替换为聚合器的值字段，并将相应的位图字段替换为聚合器的位图，然后将数据包向下游发送到PS</li></ul></li><li>ATP选择将完整的聚合结果转发给PS，而不是将它们发送回worker</li></ul></li><li>Aggregator Deallocation using Parameter Packets使用参数包进行聚合器释放<ul><li>当交换机从PS接收到参数包时，会将参数包多播回工作节点。参数包作为梯度片段包的确认(ACK)，并且必须遍历用于聚合的边缘交换机。当ATP交换机处理一个参数包时，交换机会检查该包索引处的聚合器是否具有匹配的标识符，如果匹配，则通过将所有字段更改为空来释放该聚合器</li></ul></li></ul></li><li><h3 id="end-host-logic终端主机逻辑">End HostLogic(终端主机逻辑)</h3><ul><li>Worker Pushing Gradients Worker推送梯度<ul><li>ATP终端主机网络栈通过拦截DT作业的推送或拉取调用来获取梯度。它将这些梯度分成一系列306B的数据包（58B头部+ 248B梯度值）</li></ul></li><li>PS Updating Parameters PS更新参数<ul><li>ATP在PS上为每个作业分配一块内存区域，用于收集聚合梯度，形式为由序列号索引的&lt;位图，值&gt;数组</li><li>位图跟踪哪些worker的梯度片段已在值字段中聚合。PS为每个值维护一个位图，以跟踪哪些worker的值已被聚合</li><li>当梯度片段数据包到达时，PS将其位图与数据包的位图进行比较，以查找重叠<ul><li>如果它们不重叠，则PS将数据包的数据聚合到其存储的值中，并从数据包的位图更新存储的位图</li><li>若重叠，丢弃重复项</li></ul></li><li>完成参数片段的聚合后，PS将更新后的参数片段发送到交换机，交换机将其多播回作业中的所有worker</li><li>对于单个梯度片段，当最初的几个数据包到达时，聚合器可能正忙（哈希冲突），但对于后面的数据包则可用（已释放）。在这种情况下，最初的几个数据包会直接转发到PS，而剩余的数据包则在交换机处聚合</li><li>如果没有干预，交换机将永远不会发送聚合后的值，因为它正在等待已经发送的数据包。当工作节点接收到更高序列片段的参数数据包时，它们会检测到这种停滞的聚合，并且所有工作节点都会将停滞的片段视为丢失。每个工作节点都会重新发送带有重发位设置的停滞片段</li><li>为了减少聚合器冲突的频率，我们提出了一种动态哈希方案。PS检查梯度数据包的冲突位。如果冲突位被设置，PS会重新哈希以获得一个新的聚合器索引。它将这个新的聚合器索引在参数数据包中未使用的位图字段中发送给worker</li></ul></li><li>Worker Receiving Parameters 工作节点接收参数<ul><li>网络堆栈维护一个关于梯度片段数据包序列的滑动窗口。在发送初始窗口的数据包后，工作节点将第一个未被确认的序列号记录为期望的序列号，并等待来自参数服务器（PS）的参数数据包。工作节点使用来自PS的参数数据包来滑动窗口并发送新的梯度数据包</li><li>当工作节点接收到一个参数数据包时，它会检查该数据包是否具有期望的数据包序列号<ul><li>如果该参数数据包已被接收,则会被忽略</li><li>如果它具有期望的序列号，则工作节点会增加期望的序列号，并调用拥塞控制算法来更新当前窗口<ul><li>如果正在传输的数据包数量小于拥塞窗口，ATP将发送剩余窗口（拥塞窗口大小- 正在传输的数据包数量）的梯度片段数据包</li></ul></li><li>如果参数的序列号高于预期，ATP可能会认为期望的梯度片段丢失，从而触发丢失恢复</li></ul></li></ul></li></ul></li><li><h3 id="reliability-and-congestion-control可靠性和拥塞控制">Reliability andCongestion Control(可靠性和拥塞控制)</h3><ul><li>Reliability 可靠性<ul><li>当worker接收到三个连续的、序列号不是期望序列号的参数包时，它会检测到具有期望序列号的梯度片段丢失</li><li>ATPworker会重传丢失的片段包，并设置重发位；这向交换机表明交换机中可能存在部分聚合状态</li><li>在第一级，当重传数据包到达时，交换机检查是否存在匹配的聚合器<ul><li>如果存在，并且聚合器位图未指示重传数据包的片段已被聚合，则交换机将数据包中的值聚合到聚合器中，将数据包中的位图合并到聚合器的位图中，将结果（可能是部分的）向下游转发，并释放聚合器</li></ul></li><li>第二层交换机丢弃其聚合状态，并将任何重发的包（包括来自第一层的部分聚合）转发到PS，在那里最终完成聚合</li><li>在每个参数包上，交换机都会检查由参数包索引指定的寄存器的超时值。即使其作业ID和序列号与参数包不匹配，如果时间戳早于配置的值，交换机也会释放聚合器</li></ul></li><li>Congestion Control 拥塞控制<ul><li>在多租户网络中，多个ATP作业和其他应用程序共享网络。它们争夺各种资源，包括网络带宽、接收器CPU和交换机缓冲区。在ATP中，多个作业还会争夺交换机上的聚合器</li><li>交换机中聚合器的高度争用可能导致聚合器无法聚合所有流量。这会导致流量增加，从而触发交换机中的队列长度累积以及由于交换机缓冲区溢出而导致的丢包</li><li>启用交换机中的ECN标记，并使用ECN和（罕见的）数据包丢失作为拥塞信号。为了确保ECN标记在聚合期间不丢失，ATP将分片数据包中的ECN位合并到聚合器中的ECN位，该ECN位稍后在聚合完成后转发到PS。然后，此ECN位被复制到参数数据包，并最终到达所有worker</li><li>每个ATP工作进程应用加性增量乘性减量（AIMD）来调整其窗口大小，以响应拥塞信号</li><li>对于每个收到的参数包，ATP将窗口大小增加一个MTU（1500字节或5个数据包），直到达到一个阈值。高于慢启动阈值时，ATP每个窗口将窗口大小增加一个MTU</li><li>当一个工作进程通过参数ACK上的ECN标记或三个乱序ACK检测到拥塞时，它会将窗口大小减半，并将慢启动阈值更新为更新后的窗口大小</li></ul></li></ul></li><li><h3 id="dealing-with-floating-point浮点数处理">Dealing with FloatingPoint(浮点数处理)</h3><ul><li>ATP通过将浮点数乘以一个比例因子 (108)并四舍五入到最接近的整数，将每个worker处的梯度值从32位浮点表示转换为32位整数表示</li><li>交换机聚合这些32位整数，PS通过除以比例因子将聚合值转换回32位浮点数</li><li>所有梯度数据包都以在网络交换机上进行聚合为目的发送。如果一个梯度数据包在交换机中的聚合器上触发溢出，我们利用交换机的一个特性（饱和）将聚合器的值设置为用32位整数表示的最大值或最小值</li><li>果聚合器的值已饱和，则任何进一步发送到该聚合器的梯度数据包仅更新方向，并且该值保持饱和。当聚合完成时，即fanInDegree值等于worker的数量时，饱和的聚合器值被写入梯度数据包并发送到PS。如果PS发现聚合器的值已饱和，它会从所有worker请求原始的浮点格式的梯度值。这会触发重传，并且所有worker都将包含浮点梯度值的包直接发送到PS，PS最终执行聚合</li></ul></li></ul><h1 id="implementation-实现">4. Implementation 实现</h1><ul><li>Programmable Switch 可编程交换机<ul><li>该交换机实现具有用于梯度聚合的处理逻辑以及用于分配、释放和管理聚合器的控制逻辑</li><li>必须在有限的时间预算内解析整个数据包，并在有限的交换机流水线阶段中进行处理</li><li>聚合<ul><li>ATP通过对每个数据包在交换机上进行两次处理（称为两遍处理）来增加此限制，这是重提交和循环功能的混合使用</li></ul></li><li>控制逻辑<ul><li>负责检查聚合器是否可用，处理协议标志，以及更新聚合状态</li><li>为了在对寄存器的一次性访问限制内工作，ATP应用各种技术来处理复杂操作。考虑位图检查过程，这涉及到读取聚合器中的位图，然后对梯度值和位图值进行算术运算；最后，写入聚合器中的位图</li><li>ATP中解决单个数据包的一次性访问限制的另一种方法是使用两个数据包</li></ul></li></ul></li><li>End-Host Networking Stack 终端主机网络堆栈<ul><li>将 ATP 实现为 BytePS插件，该插件集成在 PyTorch、TensorFlow和MXNet中</li><li>BytePS 允许在不修改应用程序的情况下使用 ATP。ATP 在 worker 与 ATP PS通信时，拦截 worker 上的 Push 和 Pull 函数调用</li><li>Small Packet Optimizations 小数据包优化<ul><li>TSO通过将数据包分包卸载到网卡来加速数据包发送，并通过大型DMA传输来提高PCIe带宽</li><li>MP-QP使用指定多个连续数据包缓冲区的缓冲区描述符，并将网卡内存占用减少至少512倍</li></ul></li><li>ATP使用多线程来加速数据包处理</li></ul></li><li>Baseline Implementation 基线实现<ul><li>实现了一个SwitchML的原型，它使用交换机作为PS，并提供基于超时的丢包恢复机制</li><li>在终端主机上将TSO和MP-QP特性应用于SwitchML实现，以改善小包操作，但不在交换机上应用两阶段优化，以与SwitchML的公共版本保持一致</li></ul></li></ul><h1 id="evaluation-评估">5. Evaluation 评估</h1><ul><li>实验装置<ul><li>集群设置<ul><li>8台机器配备了一块NVIDIA GeForce RTX 2080TiGPU，NVIDIA驱动版本为430.34，CUDA版本为10.0</li><li>所有机器均配备56核Intel(R) Xeon(R) Gold 5120T CPU @ 2.20GHz，192GBRAM，操作系统为Ubuntu18.04，Linux内核版本为4.15.0-20。每台主机都配备一个MellanoxConnectX-5双端口100G NIC，使用Mellanox驱动OFED4.7-1.0.0.1。所有主机通过一个具有BarefootTofino芯片的32x100Gbps可编程交换机连接</li></ul></li><li>Baseline 基线<ul><li>BytePS</li><li>SwitchML</li><li>具有RoCE的Horovod</li><li>具有TCP的Horovod</li></ul></li><li>Workloads 工作负载<ul><li>DT 作业有 8 个 worker。对于大多数实验使用 VGG16（模型大小 528MB）和ResNet50（模型大小98MB），分别作为网络密集型和计算密集型工作负载的代表</li><li>还运行一个聚合微基准测试，其中每个 worker 重复传输 4MB 张量（BytePS支持的最大大小），这些张量在网络 (ATP) 中或在 PS(s) (BytePS)中聚合，然后发送回worker。与真实作业相比，此微基准测试具有大小相等的张量，并且始终有数据要发送，没有“关闭”阶段</li></ul></li><li>Metrics 指标<ul><li>DT作业的训练吞吐量</li><li>达到目标准确率的时间</li><li>微基准测试的聚合吞吐量</li></ul></li></ul></li><li>单任务性能<ul><li>ATP训练性能<ul><li>对于所有作业，ATP均实现了最佳性能，在网络密集型工作负载（VGG）上，性能提升比在计算密集型工作负载（ResNet）上更大</li><li><figure><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/5.png" alt="Single job throughput"><figcaption aria-hidden="true">Single job throughput</figcaption></figure></li></ul></li><li>机架间聚合<ul><li>ATP在发送到PS之前，在交换机<span class="math inline"><em>S</em><em>W</em><sub>2</sub></span>处聚合来自<span class="math inline"><em>w</em><sub>5</sub></span>和<span class="math inline"><em>w</em><sub>4</sub></span>的单个数据包以及来自<span class="math inline"><em>S</em><em>W</em><sub>0</sub></span>和<span class="math inline"><em>S</em><em>W</em><sub>1</sub></span>的部分聚合；这消除了到PS的<span class="math inline">$\frac{2}{3}$</span>的流量</li></ul></li><li>丢包恢复开销<ul><li>当丢包率增加时，ATP的性能会优雅地降低，且降低程度小于SwitchML。这是因为ATP采用乱序ACK作为丢包信号，这使得ATP能够比SwitchML更快地检测和响应丢包</li><li><figure><img src="/2025/07/16/ATP%EF%BC%9A%E5%A4%9A%E7%A7%9F%E6%88%B7%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C%E5%86%85%E8%81%9A%E5%90%88/6.png" alt="Throughput in different packet loss rate"><figcaption aria-hidden="true">Throughput in different packet lossrate</figcaption></figure></li></ul></li></ul></li><li>ATP Time-to-Accuracy (TTA)ATP 准确率时间<ul><li>单任务TTA<ul><li>对于所有模型，ATP花费相同数量的epoch来达到与BytePS NtoNRDMA相同的top-5准确率</li><li>对于VGG16，一种网络密集型工作负载，ATP优于BytePS</li><li>ATP没有加速ResNet50的训练，因为它是一种计算密集型工作负载</li></ul></li><li>多任务TTA<ul><li>与单作业 TTA 性能类似，ATP 优于 BytePS 和 Horovod</li></ul></li><li>ATP不会影响训练质量，并且由于网络内聚合提供的加速作用，它能够在比其他方法更短的时间内达到基线训练精度</li></ul></li><li>多重作业<ul><li>动态 vs. 静态共享<ul><li>在 100% 的情况下，动态方法与静态方法的性能相似</li><li>随着可用聚合器数量的减少，动态方法的吞吐量下降幅度小于静态方法</li></ul></li><li>ATP哈希方案的有效性<ul><li>ATP的基于哈希的方案与基准静态方案相匹配，并且大大优于基于线性的方案</li><li>动态哈希函数可以有效地将聚合器分配给每个作业</li></ul></li></ul></li><li>拥塞控制的有效性<ul><li>With non-ATP trafﬁc 非ATP流量<ul><li>有ATP的VGG16作业能够达到峰值吞吐量（因为需求小于公平份额），并且吞吐量之和接近该工作节点上行链路的线路速率。这表明，在这种设置下，ATP的CC能够与具有最大-最小公平分配的非ATP后台流量共存</li><li>ATP能够接近瓶颈链路（来自该工作节点的上行链路）的公平份额，并且两种流量类型的吞吐量之和接近链路速率。这展示了近乎公平的链路带宽共享</li></ul></li><li>With other ATP trafﬁc ATP流量<ul><li>ATP的拥塞控制能够有效地维持高吞吐量，并且能够有效地避免丢包</li></ul></li></ul></li></ul><h1 id="other-related-work-其他相关工作">6. Other Related Work其他相关工作</h1><ul><li>Speedup Network Transmission 加速网络传输<ul><li>更智能的网络调度<ul><li>通过细粒度张量传输调度（按层而不是整个梯度或参数）来增加GPU/CPU计算和网络传输之间的重叠</li><li>通过流水线结合模型并行和数据并行</li><li>使用异步IO</li></ul></li><li>减少网络流量<ul><li>用大批量大小来降低通信频率</li><li>使用量化或减少SGD中的冗余来减少网络传输的字节数</li><li>优化本地-全局聚合的混合，以适应运行时的网络变化</li></ul></li></ul></li><li>In-network Aggregation 网络内聚合<ul><li>已在无线网络中、在大数据系统和使用终端主机的分布式训练系统中、专用主机、高性能中间盒或覆盖网络中进行了探索</li><li>DAIET提出了一个简单的网络内聚合的概念验证设计，但没有测试平台原型</li><li>ShArP在MellanoxInfiniband专用交换机的支持下，构建了一个覆盖缩减树来聚合通过它的数据，但它直到交换机接收到所有数据后才应用聚合</li></ul></li></ul><h1 id="conclusion-结论">7. Conclusion 结论</h1><ul><li>成功构建了一个网络内聚合服务ATP，以加速多租户多机架网络中的DT作业</li><li>对于单个作业，ATP的性能优于现有系统高达8.7X，甚至略优于当前最先进的基于RDMA的环状all-reduce</li><li>在多租户场景中，使用ATP的尽力而为的网络内聚合能够实现高效的交换机资源利用，并且在交换机资源竞争激烈时，在训练时间方面优于当前最先进的静态分配技术高达38%</li></ul>]]></content>
    
    
    <categories>
      
      <category>在网计算</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>在网计算</tag>
      
      <tag>ATP服务</tag>
      
      <tag>2025</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于网内聚合的分布式机器学习加速策略研究</title>
    <link href="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/"/>
    <url>/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要">摘要</h1><ul><li>研究背景<ul><li>大规模神经网络训练需求激增，单机训练效率低，而PS-Worker、AllReduce存在显著缺陷<ul><li>PS-Worker：参数服务器带宽瓶颈限制规模扩展</li><li>AllReduce：通信延迟随节点数线性增长</li></ul></li><li>网内聚合（ In-Network Aggregation）成为了加速分布式机器学习训练的新方向<ul><li>通过可编程交换机将梯度聚合卸载到网络层，减少主机计算压力与网络流量</li><li>受限于交换机内存与计算能力</li></ul></li></ul></li><li>核心问题<ul><li>问题1:<ul><li>多任务争抢可编程交换机有限的内存资源，而导致网内聚合速度减慢和交换机内存利用率降低</li></ul></li><li>问题2<ul><li>网内聚合扩展规模受限于可编程交换机有限的计算能力，大规模分布式训练中部署难度和成本高，以及现有网内聚合扩展策略没有充分利用机内高带宽资源</li></ul></li></ul></li><li>解决方案<ul><li>问题1：RA-INA混合同步算法<ul><li>动态共享交换机内存，梯度分组后优先执行Ring-AllReduce</li><li>数据包到达交换机时批量抢占聚合器，成功则切换网内聚合</li></ul></li><li>问题2：CINA链式扩展策略<ul><li>将 ToR（Top of Rack）交换机替换为可编程交换机，在 ToR层形成一条网内聚合的链式流水线</li><li>利用机内高带宽，在机内进行 Ring-Reduce将梯度聚合到机内主节点上，协同流水线进行机间的网内聚合</li></ul></li></ul></li><li>关键词<ul><li>分布式机器学习</li><li>可编程交换机</li><li>网内聚合</li><li>多任务</li><li>大规模训练</li></ul></li></ul><h1 id="第一章-绪论">第一章 绪论</h1><ul><li>研究背景及意义<ul><li>核心问题<ul><li>大规模神经网络训练（如ChatGPT）需求激增，单机训练耗时过长（如1024块A100训练34天），分布式训练成关键技术。</li></ul></li><li>通信瓶颈<ul><li>PS-Worker​<ul><li>参数服务器（PS）节点多对一通信导致带宽瓶颈</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/1.png" alt="PS架构瓶颈"><figcaption aria-hidden="true">PS架构瓶颈</figcaption></figure></li></ul></li><li>AllReduce<ul><li>通信延迟随节点数线性增长，长环结构受限于最低带宽链路​</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/2.png" alt="AllReduce瓶颈"><figcaption aria-hidden="true">AllReduce瓶颈</figcaption></figure></li></ul></li></ul></li><li>网内聚合价值<ul><li>通过可编程交换机卸载梯度聚合，减少网络流量与通信跳数，但受限于交换机内存与计算能力</li></ul></li></ul></li><li>研究现状<ul><li>中心化并行训练（PS-Worker）<ul><li>工作流程<ul><li>作为中心化的 PS 将在本地维护一个全局模型， 负责更新 Worker上的本地模型。 在每次迭代训练中，训练数据集会被分割并分配给每个 Worker进行本地训练，完成训练后计算梯度值并将其推送给 PS。 PS 收到来自各个Worker 的梯度后，使用随机梯度下降法或其他优化算法更新全局模型。随后Worker 会从 PS 拉取最新的模型以便进入下一次迭代训练。</li></ul></li><li>演进历程<ul><li>第一代<ul><li>基于Memcached的分布式参数存储</li></ul></li><li>第二代<ul><li>DistBelief</li></ul></li><li>第三代<ul><li>PS-Lite通用架构</li></ul></li></ul></li><li>优化方案<ul><li>数据/模型并行<ul><li>SINGA</li><li>CNTK</li></ul></li><li>重叠计算/通信<ul><li>WFBP</li><li>BytePS</li></ul></li><li>弹性参数服务器<ul><li>EPS</li><li>Pathways</li></ul></li></ul></li></ul></li><li>去中心化并行训练（AllReduce）<ul><li>工作流程<ul><li>不同于参数服务器模式需要 PS 和Worker 两类工作节点，在 AllReduce模式中只需要 Worker 节点， 模型参数或梯度只在 Worker 之间传输，每个Worker 都是平等的</li><li>将所有 Worker连接成一个逻辑环，每个 Worker把本地计算得到的梯度划分成 N份并依次把自己的梯度同步给下一个邻居Worker，总共经过 2*(N-1)轮同步，才能够完成所有 Worker 的梯度更新</li></ul></li><li>优化方案<ul><li>分层同步<ul><li>Hierarchical-AllReduce</li><li>2D-Torus AllReduce</li><li>HiPS</li></ul></li><li>异构问题<ul><li>BlueConnect</li><li>Blink</li><li>FlexReduce</li><li>DS-Sync</li></ul></li></ul></li></ul></li><li>网内聚合技术<ul><li>基于可编程网络设备（如可编程交换机、 FPGA 或智能网卡等）的计算能力加速各种聚合应用</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/3.png" alt="常见网内聚合解决方案对比"><figcaption aria-hidden="true">常见网内聚合解决方案对比</figcaption></figure></li></ul></li></ul></li><li>研究内容<ul><li>基于 Ring-AllReduce 与网内聚合的混合同步算法</li><li>大规模分布式机器学习训练的网内聚合CINA链式扩展策略</li></ul></li></ul><h1 id="第二章-相关技术概述">第二章 相关技术概述</h1><ul><li>分布式并行训练策略<ul><li>数据并行训练<ul><li>核心原理​<ul><li>数据集切分到不同计算节点，每个节点持有完整模型副本进行本地训练</li></ul></li><li>同步机制<ul><li>PS-Worker模式：Worker推送梯度至PS，PS聚合后广播更新（易带宽瓶颈）</li><li>AllReduce模式：节点间直接同步梯度（无中心节点）</li></ul></li><li>挑战<ul><li>通信开销随节点数增长，可能引发掉队者问题</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/4.png" alt="分布式数据并行训练"><figcaption aria-hidden="true">分布式数据并行训练</figcaption></figure></li></ul></li><li>模型并行训练<ul><li>核心原理<ul><li>模型按层/神经元切分到不同节点</li></ul></li><li>划分策略<ul><li>横向按层划分​<ul><li>节点负责特定网络层（层数多时适用）</li></ul></li><li>纵向跨层划分<ul><li>单层参数矩阵分块（神经元多时适用）</li></ul></li><li>混合划分<ul><li>结合横向与纵向策略</li></ul></li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/5.png" alt="分布式模型并行训练"><figcaption aria-hidden="true">分布式模型并行训练</figcaption></figure></li></ul></li></ul></li><li>网内聚合<ul><li>可编程交换机<ul><li>核心架构<ul><li>RMT（可重构匹配表）模型支撑数据包处理</li></ul></li><li>关键部件<ul><li>解析器​：提取/修改数据包头部字段</li><li>匹配部件​：执行流表规则（匹配+动作集）</li><li>编程语言：P4定义数据包处理流程</li></ul></li><li>挑战限制<ul><li>聚合流量可能高达数十个 Gb 甚至 Tb，远超 RMT 交换机的内存容量</li><li>SRAM/TCAM内存仅数十MB，需精细管理</li></ul></li></ul></li><li>网内聚合的应用<ul><li>分布式机器学习<ul><li>减少网络流量与主机计算负担</li></ul></li><li>类MapReduce应用<ul><li>使用网内聚合在交换机侧完成部分数据的规约任务， 在 Reduce阶段减少发送给 Reducer 节点的数据， 降低 Incast 现象出现的概率</li></ul></li><li>分布式存储修复<ul><li>减少数据传输量、提高修复效率、降低存储节点负载</li></ul></li></ul></li></ul></li><li>集合通信及分布式训练框架<ul><li>集合通信<ul><li>Broadcast<ul><li>单节点数据分发至所有节点</li></ul></li><li>Gather/AllGather​<ul><li>收集所有节点数据</li></ul></li><li>Reduce/AllReduce<ul><li>跨节点数据聚合（求和/最大值等）</li></ul></li><li>Scatter/Reduce-Scatter<ul><li>数据分块分发与局部聚合</li></ul></li></ul></li><li>分布式训练框架<ul><li>TensorFlow​<ul><li>数据流图架构，支持gRPC通信</li><li>高级API与低级API灵活适配</li></ul></li><li>PyTorch<ul><li>动态计算图，支持GLOO/MPI/NCCL通信后端</li></ul></li><li>MXNet<ul><li>跨语言支持，轻量级分布式训练</li></ul></li></ul></li></ul></li></ul><h1 id="第三章-基于-ring-allreduce-与网内聚合的混合同步算法设计">第三章基于 Ring-AllReduce 与网内聚合的混合同步算法设计</h1><ul><li>问题分析<ul><li>Ring-AllReduce 同步算法<ul><li>算法逻辑<ul><li>所有计算节点在逻辑拓扑上形成一个环，计算节点反向传播结束得到梯度后，会将梯度均匀切分成n份。每个节点错开将其中一份梯度发送给右邻居，并接收左邻居的一份梯度与本地梯度进行聚合。然后每个节点再将上一步聚合好的一份梯度继续发送给右邻居，重复上一步的操作。在进行n-1步后，每个节点都将得到一份聚合了所有节点梯度的结果。之后每个节点继续将聚合结果发送给右邻居，同时接收左邻居的聚合结果并覆盖本地位置的梯度。然后每个节点再将上一步接收的聚合结果继续发送给右邻居，重复上一步的操作。在进行n-1 步后，每个节点都将得到整份聚合了所有节点梯度的结果</li></ul></li><li>优势<ul><li>通信量固定（不超过2倍模型大小）</li></ul></li><li>缺陷<ul><li>通信延迟随节点数线性增长</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/6.png" alt="Ring-AllReduce 同步算法示意图"><figcaption aria-hidden="true">Ring-AllReduce同步算法示意图</figcaption></figure></li></ul></li><li>网内聚合算法<ul><li>算法逻辑<ul><li>计算节点将梯度打包发送给可编程交换机，可编程交换机接收到梯度数据包之后寻找到对应的聚合器进行聚合。当聚合器识别到聚合完所有计算节点的梯度数据包之后，将对聚合结果进行打包并广播给所有计算节点</li></ul></li><li>优势<ul><li>将主机侧的聚合操作卸载到交换机侧，在网络中减少聚合流量，降低通信开销</li></ul></li><li>缺陷<ul><li>多任务争抢交换机内存导致效率下降</li></ul></li><li>内存分配机制对比<ul><li>静态固定内存分配<ul><li>每个任务在可编程交换机中平均分配一块内存，任务之间相互隔离，互不影响</li><li>内存利用率低</li></ul></li><li>动态共享内存分配<ul><li>内存由多个任务共享，每个任务的梯度数据包按照先到先服务的机制占用聚合器。此外ESA 还能通过优先级机制抢占已被占用的聚合器</li><li>需要PS节点容错，有可能将回退成 PS-Worker 的训练模式</li></ul></li><li>在多训练任务的场景下采用动态共享内存分配模式更能充分发挥网内聚合的性能</li></ul></li></ul></li></ul></li><li>混合同步算法设计(RA-INA)<ul><li>整体概述<ul><li>主要流程<ol type="1"><li>初始化工作，在同步开始时，可编程交换机将内存划分成大小相同的聚合器，每个聚合器负责一组梯度的聚合任务。计算节点将梯度切分成大小与聚合器内存相同的梯度块，并将梯度块分组，然后对按组将梯度块打包成梯度数据包发送到可编程交换机中进行网内聚合</li><li>当某个节点的梯度数据包在可编程交换机中寻找到连续的 n个聚合器时，该梯度数据包将批量占领这 n个聚合器，并将梯度缓存在对应的聚合器中</li><li>部分梯度块将转而执行网内聚合算法，并在完成所有计算节点的梯度聚合工作之后，可编程交换机会将聚合结果广播给每个计算节点</li><li>若没有在可编程交换机中寻找到满足要求的聚合器组，则该梯度数据包所在的梯度组将继续执行算法的下一步</li><li>如果之后 算法执行到 n-1 步之后，即AllGather 操作时，该梯度组将不再尝试寻找空闲聚合器， 继续完成 AllGather 操作</li></ol></li><li>核心理念<ul><li>动态混合执行<ul><li>梯度同步过程在 ​Ring-AllReduce​ 与​网内聚合间动态切换，通过交换机内存占用状态决策执行路径</li></ul></li><li>批量抢占机制<ul><li>梯度数据包到达交换机时，尝试批量占领与计算节点数相等的聚合器组（非单个抢占），成功则切换网内聚合</li></ul></li></ul></li><li>过程示例<ul><li>单任务<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/7.png" alt="单任务时 RA-INA 算法的同步过程示例"><figcaption aria-hidden="true">单任务时 RA-INA算法的同步过程示例</figcaption></figure></li></ul></li><li>多任务<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/8.png" alt="多任务时 RA-INA 算法的同步过程示例"><figcaption aria-hidden="true">多任务时 RA-INA算法的同步过程示例</figcaption></figure></li></ul></li></ul></li></ul></li></ul></li><li>主机侧逻辑设计<ul><li>数据预处理<ul><li>梯度切分<ul><li>按 ​64个梯度元素/块切分</li><li>n个梯度块为一组（n=节点数），形成逻辑分组</li></ul></li><li>数据包格式<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/9.png" alt="梯度数据包的包格式"><figcaption aria-hidden="true">梯度数据包的包格式</figcaption></figure></li></ul></li></ul></li><li>通信控制逻辑<ul><li>滑动窗口<ul><li>每组梯度独立维护状态机，完成一组后滑动至下一组</li></ul></li><li>浮点处理<ul><li>梯度缩放为32位整数传输，交换机聚合后主机侧还原为浮点数</li></ul></li><li>ACK响应逻辑<ul><li>接收聚合结果后返回ACK，触发交换机释放聚合器</li><li>超时未收到ACK则重传数据包</li></ul></li></ul></li></ul></li><li>交换机侧逻辑设计<ul><li>聚合器结构设计<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/10.png" alt="聚合器结构"><figcaption aria-hidden="true">聚合器结构</figcaption></figure></li></ul></li><li>处理流程<ul><li>聚合器匹配<ul><li>根据包头<code>job_id</code>和<code>group_index</code>定位聚合器组​</li><li>若未占用，遍历寻找连续n个空闲聚合器​</li></ul></li><li><strong>数据聚合</strong>​<ul><li>校验<code>bitmap</code>防止重复聚合</li><li>整型梯度累加到<code>data</code>区，更新<code>bitmap</code>和<code>counter</code></li></ul></li><li><strong>广播触发</strong><ul><li>当<code>counter == n</code>且<code>bitmap</code>全1时，广播聚合结果</li></ul></li></ul></li></ul></li><li>可靠性设计<ul><li>丢包处理机制<ul><li>主机→交换机丢包<ul><li>2MSL未收到ACK</li><li>重传梯度数据包</li></ul></li><li>交换机→主机丢包<ul><li>2MSL未收到结果ACK</li><li>交换机重发聚合结果</li></ul></li><li>ACK丢包<ul><li>重复数据包触发bitmap校验</li><li>丢弃重复包，补发ACK</li></ul></li></ul></li></ul></li><li>实验分析<ul><li>网内聚合性能影响因素分析<ul><li>内存利用率高</li><li>任务数影响小</li><li>内存不足性能衰减小</li></ul></li><li>单任务时的训练吞吐量对比<ul><li>比Ring-AllReduce ​吞吐量提升57%</li><li>比PS-Worker 加速2.0倍</li></ul></li><li>多任务时的平均任务完成时间对比<ul><li>RA-INA比SwitchML ​JCT降低25.4%</li><li>RA-INA比Ring-AllReduce ​JCT降低18.1%</li></ul></li></ul></li></ul><h1 id="第四章-大规模分布式训练的网内聚合扩展策略设计">第四章大规模分布式训练的网内聚合扩展策略设计</h1><ul><li>问题分析<ul><li>HINA分层网内聚合扩展策略<ul><li>同步流程<ul><li>首先所有服务器中的计算节点将梯度数据包向上发送到各自所在的 ToR交换机进行第一次网内聚合。 之后 ToR交换机继续向上将聚合好的梯度数据包发送给 ToR 交换机所连接的 AGG 交换机，进行第二次网内聚合。 然后 AGG交换机仍然将聚合好的梯度数据包向上发送给所连接的Core交换机进行最后一次网内聚合。 最后作为根节点的 Core交换机将得到所有计算节点梯度数据包的聚合结果， 并原路进行组播。在每一层中，该层的交换机都将进一步组播梯度数据包，最终到达每个计算节点，完成本次梯度同步</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/11.png" alt="HINA 在 Fat-Tree 网络拓扑中构建的聚合树"><figcaption aria-hidden="true">HINA 在 Fat-Tree网络拓扑中构建的聚合树</figcaption></figure></li></ul></li><li>缺陷<ul><li>需替换多层级交换机（ToR+AGG+Core），部署成本高</li></ul></li></ul></li><li>MTINA多树网内聚合扩展策略<ul><li>同步流程<ul><li>MTINA 将根据参与训练任务的计算节点分布情况，以 Core交换机为根节点初始化生成多棵聚合树。 与 HINA 中生成的聚合树相同， MTINA中每棵聚合树的叶子节点均为计算节点，非叶子节点均为具有计算功能的可编程交换机</li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/12.png" alt="MTINA 在 Fat-Tree 网络拓扑中构建的聚合树"><figcaption aria-hidden="true">MTINA 在 Fat-Tree网络拓扑中构建的聚合树</figcaption></figure></li></ul></li><li>缺陷<ul><li>构建多聚合树需全网可编程交换机，成本激增</li></ul></li></ul></li></ul></li><li>链式网内聚合扩展策略设计<ul><li>整体结构(以Spine Leaf 网络架构为例)<ul><li>网络拓扑​<ul><li>Spine-Leaf架构，仅替换Leaf层为可编程交换机</li></ul></li><li>四阶段流水线：<ol type="1"><li>机内Ring-Reduce​<ul><li>GPU间高带宽聚合梯度至Master Node</li></ul></li><li>机架内INA<ul><li>ToR交换机聚合本机架Master Node数据</li></ul></li><li>机架间链式聚合​<ul><li>ToR交换机形成流水线，跨机架迭代聚合</li></ul></li><li>结果组播<ul><li>最终聚合结果广播回Master Node</li></ul></li></ol></li></ul></li><li>机内聚合策略<ul><li>主节点选择​<ul><li>规则<ul><li>每台服务器固定选择末位GPU为Master Node</li></ul></li><li>作用<ul><li>作为机内聚合结果缓存点与机间通信代理。</li></ul></li></ul></li><li>Ring-Reduce流水线​<ul><li>执行流程<ul><li>梯度切分<ul><li>每GPU梯度分k块</li></ul></li><li>Reduce-Scatter​<ol type="1"><li><span class="math inline"><em>G</em><sub>(<em>i</em>, 0)</sub> → <em>G</em><sub>(<em>i</em>, 1)</sub></span>发送块0，同时接收<span class="math inline"><em>G</em><sub>(<em>i</em>, 3)</sub> → <em>G</em><sub>(<em>i</em>, 0)</sub></span>块3</li><li><span class="math inline"><em>G</em><sub>(<em>i</em>, 1)</sub> → <em>G</em><sub>(<em>i</em>, 2)</sub></span>发送块1，同时接收<span class="math inline"><em>G</em><sub>(<em>i</em>, 0)</sub> → <em>G</em><sub>(<em>i</em>, 1)</sub></span>块0</li><li>…</li></ol></li><li>聚合终点​<ul><li>经(n+k-2)步，所有梯度块聚合至<span class="math inline"><em>G</em><sub>(<em>i</em>, 3)</sub></span></li></ul></li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/13.png" alt="CINA 算法使用 4 个 GPU 进行 Ring-Reduce 操作的分解过程"><figcaption aria-hidden="true">CINA 算法使用 4 个 GPU 进行 Ring-Reduce操作的分解过程</figcaption></figure></li></ul></li><li>延迟公式​<ul><li><span class="math inline">$T_{机内RR} = \frac{(n+k-2)K}{kB_{dtd}} +(n+k-2)\alpha_{dtd}$</span>（n=GPU数,K=梯度大小n=GPU数,K=梯度大小n=GPU数,K=梯度大小）</li></ul></li></ul></li><li>机架内聚合策略<ul><li>ToR交换机<ul><li>核心功能<ul><li>聚合本机架内所有Master Node梯度</li><li>生成机架级聚合结果</li></ul></li></ul></li><li>通信重叠优化<ul><li>Master Node在接收第一份聚合梯度后立即发送至ToR交换机</li><li>覆盖机内剩余通信时间<ul><li><span class="math inline">$T_{重叠} \leq T_{机内RR} -\left(n-1)\times( \frac{K}{kB_{htl}} + \alpha_{htl} \right)$</span></li></ul></li></ul></li></ul></li><li>机架间聚合策略<ul><li>链式聚合原理<ul><li>Leaf交换机按Pod编号串联</li><li>工作流程<ol type="1"><li><span class="math inline"><em>L</em><em>S</em><sub>0</sub></span>：发送本地聚合结果<span class="math inline"><em>g</em>2<sub><em>t</em></sub><sup>(0)</sup></span>​至<span class="math inline"><em>L</em><em>S</em><sub>1</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>1</sub></span>​：计算<span class="math inline"><em>g</em>3<sub><em>t</em></sub><sup>(1)</sup>​ = <em>g</em>2<sub><em>t</em></sub><sup>(1)</sup>​ + <em>g</em>2<sub><em>t</em></sub><sup>(0)</sup></span>，发送至<span class="math inline"><em>L</em><em>S</em><sub>2</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>2</sub></span>​：计算<span class="math inline"><em>g</em>3<sub><em>t</em></sub><sup>(2)</sup>​ = <em>g</em>2<sub><em>t</em></sub><sup>(2)</sup>​ + <em>g</em>3<sub><em>t</em></sub><sup>(1)</sup></span>，发送至<span class="math inline"><em>L</em><em>S</em><sub>3</sub></span></li><li><span class="math inline"><em>L</em><em>S</em><sub>3</sub></span>​：获得全局聚合结果<span class="math inline"><em>g</em>3<sub><em>t</em></sub>​</span></li></ol></li></ul></li><li>通信重叠优化<ul><li><span class="math inline">$T_{CINA}=T_{机内RR}-T_{重叠}+\frac{K}{B_{htl}}+\alpha_{htl}+(L-1)\times(\frac{K}{kB_{lts}})+\frac{K}{kB_{min}}+\alpha_{min}$</span></li></ul></li></ul></li></ul></li><li>仿真分析<ul><li>理论通信开销推导与对比<ul><li>Ring-AllReduce<ul><li><span class="math inline">$T_{Ring}=2(N-1)\times(\frac{K}{NB_{min}}+\alpha_{min})$</span></li></ul></li><li>PS-Worker<ul><li><span class="math inline">$T_{PS}=(k+1)\times(\frac{K}{kB_{min}})+2\alpha_{min}$</span></li></ul></li><li>HINA<ul><li><span class="math inline">$T_{HINA}=T_{机内RR}-T_{重叠}+\frac{K}{B_{htl}}+\alpha_{htl}+2\times(\frac{K}{kB_{lts}}+\alpha_{lts})+\frac{K}{kB_{min}}+\alpha_{min}$</span></li></ul></li><li>MTINA<ul><li><span class="math inline">$T_{MTINA}=(k+1)\times(S+1)\times(\frac{K}{kSB_{min}})+(S+1)\times\alpha_{min}$</span></li></ul></li><li>优势对比<ul><li>CINA只在服务器与上层交换机间存在多对一通信的压力，并不会一层一层的积累通信压力</li><li>CINA 不需要像 HINA 和 MTINA 中将 Leaf 层和 Spine层的交换机替换为可编程交换机，本设计只需要更换 Leaf层的交换机，降低了部署难度和成本</li><li>三层网络最大支持节点数：​CINA 14,338 &gt; HINA 3,600</li><li>相同的节点规模下， CINA 的部署成本也比 HINA 和 MTINA 更有优势</li></ul></li></ul></li><li>仿真平台搭建<ul><li>平台架构设计​<ol type="1"><li>真实VGG19/ResNet50训练数据</li><li>NS3构建可扩展Spine-Leaf网络。</li><li>实现五种对比算法</li><li>丢包率与聚合正确性监控</li></ol></li><li>关键参数配置<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/18.png" alt="仿真参数配置"><figcaption aria-hidden="true">仿真参数配置</figcaption></figure></li></ul></li><li>Trace激励源设计​<ul><li>PyTorch Profiler + 自定义Hook</li></ul></li><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/19.png" alt="仿真平台搭建流程"><figcaption aria-hidden="true">仿真平台搭建流程</figcaption></figure></li></ul></li><li>仿真结果与分析<ul><li>训练时间对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/14.png" alt="VGG19 和 ResNet50 的训练时间对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练时间对比</figcaption></figure></li></ul></li><li>吞吐量对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/15.png" alt="VGG19 和 ResNet50 的训练吞吐量对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练吞吐量对比</figcaption></figure></li></ul></li><li>拓展效率对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/16.png" alt="VGG19 和 ResNet50 的训练拓展效率对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练拓展效率对比</figcaption></figure></li></ul></li><li>加速比对比<ul><li><figure><img src="/2025/07/05/%E5%9F%BA%E4%BA%8E%E7%BD%91%E5%86%85%E8%81%9A%E5%90%88%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E7%AD%96%E7%95%A5%E7%A0%94%E7%A9%B6/17.png" alt="VGG19 和 ResNet50 的训练加速比对比"><figcaption aria-hidden="true">VGG19 和 ResNet50的训练加速比对比</figcaption></figure></li></ul></li></ul></li></ul></li></ul><h1 id="第五章-总结与展望">第五章 总结与展望</h1><ul><li>总结<ul><li>RA-INA混合同步算法<ul><li>解决痛点<ul><li>多任务争抢交换机内存导致的效率下降</li></ul></li><li>工作机制<ul><li>动态共享内存<ul><li>任务按梯度组抢占聚合器</li></ul></li><li>混合执行逻辑​<ul><li>Ring-AllReduce容错 + 网内聚合加速</li></ul></li></ul></li></ul></li><li>CINA链式扩展策略<ul><li>解决痛点<ul><li>大规模分布式训练场景下网内聚合的扩展性问题</li></ul></li><li>工作机制<ul><li>多阶段流水线<ul><li>机内Ring-Reduce → ToR层链式聚合</li></ul></li><li>扁平化的聚合结构</li></ul></li></ul></li></ul></li><li>展望<ul><li>同步算法对丢包容忍性</li><li>跨交换机甚至跨机架的实机实验平台</li><li>网内聚合应用在分布式模型并行训练场景，研究异步训练下的网内聚合算法设计</li><li>对于其他的可编程网络设备研究其网内聚合的分布式机器学习加速策略</li></ul></li></ul>]]></content>
    
    
    <categories>
      
      <category>在网计算</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>在网计算</tag>
      
      <tag>2025</tag>
      
      <tag>网内聚合</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于在网计算的分布式系统加速方法</title>
    <link href="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/"/>
    <url>/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/</url>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><ul><li>背景<ul><li>在网计算是后摩尔时代提升算力的有效方法</li><li>分布式系统扩展面临功耗墙、存储墙，搬动数据开销达到整体70%</li></ul></li><li>定义<ul><li>在网计算是将标准应用卸载到网络设备上的计算模式</li><li>In-Network Computing is the offloading of standard applications torun within network devices</li></ul></li><li>意义<ul><li>压缩网络流量，低时延通信，高速计算</li><li>适用于分布式应用加速，如：分布式大模型训练、分布式数据分析、分布式存储系统</li></ul></li><li>与传统方案对比<ul><li>传统方案的现状<ul><li>分层结构各层独立，各自优化，形成生态，性能无法达到最优</li><li>应用异构性较大，形成各自的通信模式</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/1.png"></li></ul></li><li>在网计算的发展空间<ul><li>大型集群计算成为新趋势，集群归一方所有，分层设计的前提不再具备</li><li>各层具备可编程性、可定制性</li><li>集群建设成本高昂，提升效率意义重大 <img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/2.png"></li></ul></li></ul></li><li>发展路线<ul><li>打通二至五层，各应用领域独立设计；在各个应用领域验证并取得收益</li><li>领域间逐步整合，重新设计通信库标准；使应用开发者迁移到新的开发模式</li><li>功能下沉，更新设备，进一步提升性能；促进大规模商业化生产</li></ul></li></ul><h2 id="技术方案">技术方案</h2><ol type="1"><li><p>加速分布式机器学习 【前置信息】</p><ul><li>背景<ul><li>机器学习：自然语言处理、计算机视觉、智能运维</li><li>分布式机器学习：增长的数据集、增长的模型、场景要求(联邦学习)</li></ul></li><li>机器学习训练算法<ul><li>重复迭代计算梯度，更新模型直至收敛</li><li>分布式训练：每个worker计算梯度 –&gt; 所有梯度聚合并返回给worker–&gt; 更新模型</li></ul></li><li>分布式训练参数服务器（Parameter Server, PS）<ul><li>假设：N个worker，梯度大小为M</li><li>通信量 worker: M PS: N * M 【问题与目标】</li></ul></li><li>问题<ul><li>PS链路成为瓶颈</li></ul></li><li>目标<ul><li>主要目标：使用在网计算加速分布式训练</li><li>其他目标：兼容性、多任务、跨机柜</li></ul></li></ul><p>【方案1】交换机取代PS (NetReduce)</p><ul><li>目标<ul><li>主要目标： 使用在网计算加速分布式训练</li><li>其他目标：与RDMA兼容</li></ul></li><li>PS的类型及工作方式<ul><li>服务器<ol type="1"><li>每个worker把数据发送到PS</li><li>PS把数据求和</li><li>PS将结果广播给worker</li></ol></li><li>交换机<ol type="1"><li>交换机能流式处理报文，但是不能缓存大块数据</li><li>交换机没有四层协议保证数据块完整</li></ol></li></ul></li><li>体系结构<ul><li>Worker：梯度消息被组织为一个报文序列，维护一个滑动窗口发送报文，假设窗口最大值为W</li><li>交换机：内存组织为一个聚合器数组，大小为N <img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/3.png"></li></ul></li><li>工作流程<ol type="1"><li>Worker发送梯度报文，交换机通过聚合器数组实时聚合（agtr.idx =PSN%N）</li><li>一个聚合器中的聚合完成（通过bitmap判断），将结果组播给workers</li><li>worker的滑动窗口继续滑动，发送新的报文</li></ol></li><li>性能结果<ul><li>NetReduce提升训练效率，获得在网聚合和RDMA双重性能增益</li><li>训练AlexNet比Ring AllReduce快45%</li></ul></li><li>小结<ul><li>可以用交换机替换PS</li><li>可以做到对传输层透明，与RDMA集成</li><li>内存管理：交换机内存为端侧窗口两倍</li></ul></li></ul><p>【方案2】 聚合传输协议设计ATP</p><ul><li>背景<ul><li>多租户：多任务共享基础设施</li><li>多机柜：BERT-Large训练跨多机柜</li></ul></li><li>目标<ul><li>主要目标：使用在网计算加速分布式训练</li><li>次要目标：支持多租户任务、支持跨机柜部署</li></ul></li><li>内存分配类型<ul><li>静态交换机内存分配<ul><li>划分交换机内存为隔离的区域，分配给多任务</li><li>交换机内存低效率、集成复杂</li></ul></li><li>动态交换机内存分配<ul><li>交换机内存为一个聚合器资源池，对于每个任务的报文采用先到先服务</li><li>去中心化聚合器寻址</li><li>Agtr.idx = Hash(JobID, PSN)</li></ul></li></ul></li><li>体系结构<ul><li>保留PS，寻址可能失败，需要回退机制（Fallback），交换机难以正确处理重传报文</li><li>聚合器，记录JobID、Seq，检测冲突，释放需要由ACK完成</li></ul></li><li>工作流程<ol type="1"><li>worker端维护滑动窗口，发送梯度报文</li><li>梯度报文抵达交换机，进行寻址</li><li>PS接到聚合结果或者透传报文，完成聚合，返回ACK</li><li>ACK抵达交换机，释放聚合器,推动滑动窗口</li></ol></li><li>性能结果<ul><li>通信瓶颈任务吞吐量提升显著，多任务竞争下资源利用率更高</li></ul></li><li>小结<ul><li>统计复用交换机，利用率更高</li><li>正确性机制更复杂</li></ul></li></ul><p>【方案1与方案2对比】</p><ul><li>组成<ul><li>方案1：交换机</li><li>方案2：交换机+PS</li></ul></li><li>寻址机制<ul><li>方案1：冲突避免(模运算)</li><li>方案2：冲突探测(Hash)</li></ul></li><li>内存使用<ul><li>方案1：2倍窗口</li><li>方案2：1倍窗口</li></ul></li><li>内存分配<ul><li>方案1：隔离</li><li>方案2：共享</li></ul></li><li>适用范围<ul><li>方案1：单任务隔离</li><li>方案2：多任务共享</li></ul></li><li>优势<ul><li>方案1：可预测性能，不需PS</li><li>方案2：高资源利用率</li></ul></li></ul></li><li><p>分布式机器学习任务管理 【资源管理】</p><ul><li>应用和系统目标<ul><li>高性能，资源利用率，公平性</li></ul></li><li>策略<ul><li>循环（round robin），多级反馈队列（multi-level feedback queue）</li></ul></li></ul><p>【调度策略】</p><ul><li>最早截止时间优先（Earliest Deadline First）</li><li>优先将交换机内存分配给截止时间早的任务</li><li>剩余内存共享给BE任务和暂时无法满足的MD任务</li><li><span class="math display">$$\begin{array}{c}\text{Time}_{\text{job}} = \text{Epochs} \times \left(\text{Time}_{\text{comp}} +\dfrac{\text{Size}_{\text{model}}}{\text{Throughput}} \right)\\\\\text{Memory}_{\text{switch}} = \text{Throughput} \times\text{RTT}\end{array}$$</span></li></ul><p>【内存管理】</p><ul><li>交换机内存管理器<ul><li>北向接口：内存分配</li><li>南向接口：向终端发送内存区域(offset, size)，向交换机发送规则</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/4.png"></li></ul></li><li>寻址模式<ul><li>agtr.idx ← Hash(PSN, JobID)%Size + Offset</li><li>端侧计算地址，并封装在报文头部</li></ul></li></ul><p>【性能结果】</p><ul><li>交换机内存管理可以有效降低平均任务完成时间，提高服务满足率</li></ul></li><li><p>加速大数据分析系统</p><p>【挑战】</p><ul><li>大数据存在数据流聚合<ul><li>大数据系统中的 ReduceByKey()</li><li>数据库系统中的 sum 、 count 等</li></ul></li><li>大数据系统中的聚合为异步聚合<ul><li>同步聚合(机器学习)值流（value stream）<ul><li>键相同</li><li>键线性排列，可预知</li><li>键次数有界</li></ul></li><li>异步聚合（大数据）键值流（key-value stream)<ul><li>不同流中键不同</li><li>键无序，不可预知</li><li>键次数没有界</li></ul></li></ul></li></ul><p>【目标】</p><ul><li>目标<ul><li>适配不同的应用</li><li>提升系统效率</li><li>有正确性保证</li></ul></li></ul><p>【创新设计】</p><ul><li>提升系统有效吞吐率<ul><li>多元组报文</li><li>二维聚合器数组（Aggregator Array，AA）</li><li>端侧元组顺序重构<ul><li>将键的空间划分为互不重叠的子空间</li><li>Hash(Flow ID) –&gt;子空间</li><li>报文有多个槽位（slot）</li></ul></li><li>提升155倍处理速度</li></ul></li><li>可靠性和正确性<ul><li>基于seen状态位避免重复计算</li></ul></li><li>提升内存使用效率<ul><li>双副本AA动态切换，优先处理高频键</li></ul></li></ul><p>【性能结果】</p><ul><li>词频统计任务耗时比Spark（56核）减少50%以上，CPU利用率显著降低</li></ul></li><li><p>通信库设计 【背景】</p><ul><li>在网计算对于应用（框架）开发者并不友好，学习曲线陡峭，难以集成到分布式系统中</li><li><img src="/2025/06/29/%E5%9F%BA%E4%BA%8E%E5%9C%A8%E7%BD%91%E8%AE%A1%E7%AE%97%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%8A%A0%E9%80%9F%E6%96%B9%E6%B3%95/5.png"></li></ul><p>【目标】</p><ul><li>实现对开发者友好的通信计算库</li></ul><p>【实现方法】</p><ul><li>INC应用分类：同步聚合、异步聚合、键值读写、共识协议</li><li>统一各分类中的数据格式</li><li>统一交换机中的原语</li><li>统一应用编程接口</li><li>系统集成</li></ul><p>【性能结果】</p><ul><li>节约代码行数</li><li>对比其他INC方案性能接近，对比端侧系统吞吐量更高、时延更低</li></ul></li></ol><h2 id="总结">总结</h2><ul><li>在网计算是有效提升分布式应用效率的一种手段</li><li>在网计算应用场景广泛，也是一个很大的研究空间</li></ul>]]></content>
    
    
    <categories>
      
      <category>在网计算</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>在网计算</tag>
      
      <tag>2025</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p>今天心血来潮，搭了自己的博客，希望长更新，加油。</p><p>在这之前的文章都是我后面搬上去的，为了表示对于时间的尊重，我还是按照原时间排列。</p><p>此文章可以作为姓名墙，要是有幸被您访问，可以在评论区留下你的姓名呀~</p><p>行秋镇文底</p><figure><img src="/2025/06/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.png" alt="test"><figcaption aria-hidden="true">test</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>测试</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高考一周年的回忆</title>
    <link href="/2025/06/07/%E9%AB%98%E8%80%83%E4%B8%80%E5%91%A8%E5%B9%B4%E7%9A%84%E5%9B%9E%E5%BF%86/"/>
    <url>/2025/06/07/%E9%AB%98%E8%80%83%E4%B8%80%E5%91%A8%E5%B9%B4%E7%9A%84%E5%9B%9E%E5%BF%86/</url>
    
    <content type="html"><![CDATA[<p>2024年6月7日，一切都如往常一般，2024年6月9日，一切画上了最后的句号。</p><p>不知不觉，已经过去1年了，那个战场与我而言就是熟悉又是陌生。我不知道现在的自己是否还能和1年前的自己一样充满自信。这一年里，晚上睡觉的时候，我有时还能在梦中回到那个高三4班，回到进才楼，回到那个第一排讲台旁边的位置，回到与玉芬、喜哼、王博欢笑的场景。但这终究是梦，梦是回不去的。更让人害怕的是，随着时间的推移，能梦到的过去会越来越稀薄，直到有一天，过去在梦中也成为了过去，高考就真的只是口中的一个被赋予了各种回忆的抽象集合的代名词了。很害怕，那个时候，当我谈起自己的高考的时候，是否也会像自己的师长那般，想说却说不出任何自己的细节了。</p><p>高考，青春这场大戏的最高潮，再也回不去了。浪潮之后的我们，不再是当年那个只会做题的你我了——再见了，高考。</p><p>怀念是个美丽的骗子，它只给你看过去的种种高光，却隐藏了那些同样重要却不被人注意的细节。当你翻看高考前的旧照片，回放曾经的旧场景，然后开始说服自己——那些在高考前的种种比眼下更好。但是，高考他已经过去了，彻底地过去了，一去不复返了。你的肉体在当下，心仍然活在那个早已不存在的时光，把当下交给那些早已离去的“你我他”。而你回忆中的“你我他”早已继续了自己的生活。他们在已经没有你的故事里找到了新的平衡，而你却依旧紧握着，死死地把时光攥在手里不想让它离去。</p><p>很遗憾，上面所述的就是我。这一年以来，我一次次回忆起过去的时光，就愈发的感到后悔。如果当时这样这样，我岂不是就可以那样那样了？而这一切都是假设罢了，假设是没有结果的，他让我在一个接着一个的假设中迷失自我。</p><p>寒假与玉芬、喜哼、洛熙一起回了学紫看望老师。又走过了那个鱼龙混杂的进才楼一楼，又走过了共同偷印资料的图书馆，又走过了那个逗仓鼠、兔子的生竞小教室，又走过了偷玩电脑的化竞阶梯教室。那些都是充满了回忆的地方，可是现在呢？当年自我自习狂潮的进才一楼如今再没见到一个新人、当年偷印资料的图书馆一楼被锁上大门、曾经动物多多的场所如今只留下几个空空的笼子、而阶梯教室的黑板上也不再是当年毕韧每周带来的新奇公式。一切都已经逝去了，在那时，我的心中充满了一股空虚感。是啊，斯人斯物皆已逝，我们何时来过、有何时离去呢？</p><p>一年以后的现在，我想对一年前的自己说：“不必这么紧张，放轻松，一场小小的考试而已，决定不了什么”。那个时候在高考前一晚还在疯狂背《种树郭橐驼传》的我肯定无法理解曾经朱振东在课堂上所说的“选择大于努力”一言，一切的一切都需要自己去感悟，去体会。</p><p>让过去留在它该留的地方吧。今天，比过去更需要你！</p><p>这次仅仅只是一个高考一周年小随笔，写的很乱，也是想到那写到哪，还望见谅。</p><div style="text-align: right;">——2025年6月7日星期六1:33于西电宿舍</div>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
      <tag>高考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>首考日的回忆</title>
    <link href="/2025/01/06/%E9%A6%96%E8%80%83%E6%97%A5%E7%9A%84%E5%9B%9E%E5%BF%86/"/>
    <url>/2025/01/06/%E9%A6%96%E8%80%83%E6%97%A5%E7%9A%84%E5%9B%9E%E5%BF%86/</url>
    
    <content type="html"><![CDATA[<p>正如同金工划线，一次划成固然是好的，但当我们第一次划错了，并不代表这个工件已经无用了，相反我们可以细心地为它第二次划线。</p><div style="text-align: right;">——郑盛的最后一课</div><p>早上6点半，我一早来到了2楼实验室，一边啃着面包，一边再过最后一遍物理学史和实验题。在这之前，我虽然已经经历了大大小小好多次模考，但终究没有真正来过一次首考。现在，不到1个半小时，首考它真的来了。</p><p>早上7点出头，该进场了。我和同学们商业互吹着跨过“状元门”，与荣考最后击一下掌，哼着小曲进入了博识楼。因为学选考同时进行的关系，我还能看到好多高二的的xdx，不禁想起一年前的自己也是这样注视着前辈们走进物理考场。</p><p>7点半，我来到了自己的考场，坐到了自己的位置。一切都是如此平静，我的斜前方坐的是“丁神”。默念着“丁神祝我”，我习惯性地准备把笔袋放到抽屉了……然鹅，逆天考场检查，在我座位抽屉里赫然摆着物理教材全解和教科书！心里骂了好几遍清理考场的xdx，我举手示意了一下监考员。监考员带着笑容走了过来，也许以为是一些很平常的事情，然后我就看着她的笑容是如何快速凝固的（嘿嘿）。她也骂了一遍清考场的xdx，然后就把书全掏了出来，放到了讲台上。</p><p>7点50，发卷了，我大致翻了翻，太好了是电磁感应新情境，我们没救了！随着8点铃声一响，开始答题了，我也是拼尽全力开干选择题。嗯，这道牛二，这道平抛，这道宇宙航行，还是很轻松的，太好了是未知量估计压轴，可以看实验了。随着丁神第一个在我们考场翻卷子面了，我也开始被动能定理左一拳右，电磁感应右一脚。</p><p>9点了，高二的xdx放了。听着外面嘈杂的声音，我知道留给自己的只有半个小时了，我该如何收拾这残局呢……</p><p>中午12点，我放弃了午睡，又是啃着面包，还想做一遍镇海的压轴选择。不知怎的，做着做着就进入了经典的“一个人问你问题，然后一群人开始互相问来问去”的故事情节了。我也没有做完自己本来想要完成的静心的计划。心里想着互相提问也算背书，我也加入了聊天大群。</p><p>十二点四五十，又要进考场了，和卫捐又一次的击掌。当时我在心中还想着，这是和您最后一次见面了，拜拜了您嘞。</p><p>下午1点，怀着平静的心情再次进入考场。</p><p>1点20，看到了卷子，我大致扫了一眼，嗯，与考前想的没啥区别。</p><p>1点半，随着铃声我又开始了自己想象中的“冲锋”。很好，经典选择空了好多道。没事，不慌，大题只要不要太差还有救。就决定是你了，无机题，交出你的全部分数！然后……然后，我就被突如其来的计算题单杀了。为什么？！无机化学的部分怎么会出现计算题！可恶的出卷老登！问题不大，继续做反应原理，稳住阵脚！不是，诶呦，你干嘛……怎么要计算酸碱度了……</p><p>2点55，我绞尽脑汁，想出来2个同分异构体。</p><p>化学，不可战胜的。</p><p>刚考完化学，就马上要去考技术了。化学的失利，让我决定要好好再看看技术书。于是乎，看到了3点半。于是乎匆匆忙忙过安检。于是乎顺便和军辉和“昨晚答疑未出现”的牢郑击了下掌。</p><p>16点25，来到了信息部分的最后一个大题，我也相信自己可以摘下这颗明珠。心里想着如此，我便开始了自己的冲锋。</p><p>16点45，面对着还没有头绪的三个空，我选择了先去做通用。但正如军辉所说的那样，在做通用的时候，我脑子总是和我说“你信息最后一道大题没写出来”。但我没有办法，只能硬着头皮做。通用也是出现了新题，祖传的天人合一题目。</p><p>17点10分，杀到了通用最后一题，太好了是用非门做振荡器。想起来了，我一切都想起来了，这个结构我以前做过！等一下，丸辣，忘记咋搞了。我凭着自己的记忆，照猫画虎地画了一个。我知道，这道题，应该没什么人能做出来，毕竟这张卷子好像没多少人写过。但是信息那三个空却能决定生死。</p><p>17点28分，盯出来了，是桶，我们有救了。但是，脑子此时却突然掉线了。</p><p>17点半，技术考试结束。</p><p>2024年的1月6日便是如此结束了。我只记得自己像个孤魂一般，飘出了博识楼，来到了三楼餐厅点了一碗牛肉面。那是我记忆深刻的一天，也许是因为这碗面里面掺杂了我的眼泪吧。或许有不甘，有无奈，有遗憾，有懊悔，原来这才是首考。</p><p>2025年1月6日，恍惚间，距离我首考结束居然已经整整一年了。</p><p>如此巧合，今晚实在想不出来吃啥的我又在西北的这座大城市里给自己点了一碗牛肉面。又是18点多，又是一个人，又是这碗面，还是这团能让我眼睛蒙上白色的雾。今天的我又刚好考完了逆天的大学英语考试。</p><p>此时此刻竟正如彼时彼刻</p><div style="text-align: right;">敬我的2024年浙江首考一周年。</div><div style="text-align: right;">——晚上吃面忽然想起往事的小鲁</div><div style="text-align: right;">2025.1.6</div><div style="text-align: right;">于西电宿舍</div>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>2025</tag>
      
      <tag>高考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我的2024</title>
    <link href="/2024/12/31/%E6%88%91%E7%9A%842024/"/>
    <url>/2024/12/31/%E6%88%91%E7%9A%842024/</url>
    
    <content type="html"><![CDATA[<p>很快啊，这样就已经来到了2024年的最后一年（牢大年的最后一天了……）。其实感觉2024年的第一天还在眼前，那天我还特意去径山寺给自己首考祈福，结果偶遇马克西姆，debuff加满，拼尽全力无法战胜（）。</p><p>但这一年也是确确实实过去了，在这个2024年我又经历了无数大大小小的事情。</p><p>一月是浙江特有的首考“快乐”时间。首考也是击碎了我自己对于赋分制的美好幻想，当我还在璃月港乱逛的时候，那份1年前的成绩短信也随之而来，物化的失利的确让人破防，从此同学之间都带上了首考分数的面具。当然如此沉重的一月也是有乐子的，我们也送走了牢郑（whatcan Isay）。就这样以一个传奇人物的陨落为开头，我为期半年的六边形生涯就此开始了。</p><p>二月开始备战三一了。三位一体确实是一个很有意思的东西，我也蛮喜欢的（也许是因为只需要动嘴皮子的原因吧）。但很遗憾，尽管备战了好久三一，最终省内却只报了一个杭电，而且还因为没交报名费的原因幽默出局，现在想想确实很幽默。</p><p>三月到四月就是回归刷刷题的时间了。不同于首考备战，物化技很难再回到过去大家一起竞速的时代，转而代之的便是大数学时代。逆天数学改革，19题确实与过去练习一坤年的21题的卷子不一样，而如今在25年到来之际，我也知道了，数学卷子又双叒叕改了，不过这就是后话了。这段时间，我感觉自己又回到了高三上的时间，和大家玩的玩、笑的笑、乐的乐，玩玩原神，打打万智牌，亦或者与大家讨论讨论人win与joker的逸闻。</p><p>五月拍毕业照了。那段时间，我经常会想到，居然还有不到一个月就要与大家告别了，时间可过的真快啊。这段时间强度拉满，也算是黎明前的黑暗吧。</p><p>六月，真正的高考来了。听了快18年了，从小到大我遇到的师长同龄们都跟我说，这是一场决定人生的考试，也是你一生最公平的考试。在以前我总是会想象当自己真的坐到高考考场上时，那个时候我会怎样？而事实告诉我，我依旧是平静地坐在考场中完成了高中生涯最后六份普普通通的卷子。当我交完化学卷子后，我是一片空白的。因为它的终于结束了，那个预告了18年的大boss终于结束了。我不知道自己当时是怀揣着怎样的心情回到2楼的那个实验室，尽管当时我已经知道化学的同分异构体没有写完。结束了，一切都结束了，我终于不用为解析几何费劲心思了、终于不用每天背作文模板了、终于不用每天做化学选择了。当然之后我也简单了参加了港中深和浙大海宁的三一，不过很显然，身处西安的我已经给出了答案——重在参与。最后在我生日正正好好一个月前，浙江省教育局给我发来了2024年以来第二封短信，出成绩了。</p><p>暑假的开头被志愿填充得焦头烂额，当然也是成功得知了自己的下家——泥电。一切都结束了，至此高中的最后一页也翻过去了，别了我的紫金港。接着就是准备电专的生活了，也是采购了一大堆电子产品，玩了好多游戏（当然也稍微学了一点）。这就是传说中最幸福的暑假！而我的IP也从跟着自己18年的杭州切换至西安服务器了。</p><p>九月又是一个金秋飘香的开学季，而我的朋友们，他们暂时是见不到了，不过我也迎来了新的朋友们。在这个九月我也做了好多18年以前从来没干过的事情，也真正体验到了传说已久的大学生活。</p><p>十月是一个悲伤的月份，就不必回忆了。</p><p>十一，十二月便是在适应大学生活后的时光了。在初来到西电这片土地的兴奋劲过去后，便是我平平淡淡的生活了。乐跑40次，学生组织开会，考试学习以及积极参加各类活动，的的确确时光就是这么过去了，我竟想不出一些刻苦铭心的事情。</p><p>行文至此，我已经简单地写完了自己这一年所经历的事情了。很遗憾，正如同《三体》中三维的物体跌落到二维后便会永远地失去一些细节一般，这一年的经历远远不会像我前面所描述的那样稀稀拉拉。这一年有你我的酸甜苦辣，有你我的点点滴滴。感谢各位在我的2024年剧本中的出现，有的也许是常驻嘉宾，有的也许只相遇在转瞬之间，但正是有了各位的出现，我的2024年才会如此具体丰富。</p><p>愿星河徜徉，未来一路有光。</p><p>各位，2025见！</p><div style="text-align: right;">2024年12月31日凌晨1点50分</div><div style="text-align: right;">鲁铭康</div><div style="text-align: right;">于西安电子科技大学</div><p><img src="/2024/12/31/%E6%88%91%E7%9A%842024/1.png"></p>]]></content>
    
    
    <categories>
      
      <category>思考</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
      <tag>年度回忆</tag>
      
      <tag>2024</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
